{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a74f58b9",
   "metadata": {},
   "source": [
    "## Step 1: Project Setup and Library Imports\n",
    "\n",
    "First, we'll install and import all necessary libraries for:\n",
    "- Deep Learning (TensorFlow/Keras)\n",
    "- Image Processing (OpenCV, PIL)\n",
    "- Data Manipulation (NumPy, Pandas)\n",
    "- Visualization (Matplotlib, Seaborn)\n",
    "- Dataset Management (Kaggle API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2586616b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.5-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting pillow\n",
      "  Downloading pillow-12.0.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.8.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting albumentations\n",
      "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.13/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.13/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.13/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in ./.venv/lib/python3.13/site-packages (from tensorflow) (4.15.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-2.0.1-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Downloading keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.13/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting black>=24.10.0 (from kaggle)\n",
      "  Downloading black-25.11.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (85 kB)\n",
      "Requirement already satisfied: bleach in ./.venv/lib/python3.13/site-packages (from kaggle) (6.3.0)\n",
      "Collecting kagglesdk (from kaggle)\n",
      "  Downloading kagglesdk-0.1.13-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting mypy>=1.15.0 (from kaggle)\n",
      "  Downloading mypy-1.18.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting python-slugify (from kaggle)\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting types-requests (from kaggle)\n",
      "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting types-tqdm (from kaggle)\n",
      "  Downloading types_tqdm-4.67.0.20250809-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: PyYAML in ./.venv/lib/python3.13/site-packages (from albumentations) (6.0.3)\n",
      "Collecting pydantic>=2.9.2 (from albumentations)\n",
      "  Downloading pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Collecting albucore==0.0.24 (from albumentations)\n",
      "  Downloading albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
      "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
      "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
      "  Downloading stringzilla-4.3.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (110 kB)\n",
      "Collecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
      "  Downloading simsimd-6.5.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (70 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting click>=8.0.0 (from black>=24.10.0->kaggle)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting mypy-extensions>=0.4.3 (from black>=24.10.0->kaggle)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pathspec>=0.9.0 (from black>=24.10.0->kaggle)\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: platformdirs>=2 in ./.venv/lib/python3.13/site-packages (from black>=24.10.0->kaggle) (4.5.0)\n",
      "Collecting pytokens>=0.3.0 (from black>=24.10.0->kaggle)\n",
      "  Downloading pytokens-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Downloading optree-0.18.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic>=2.9.2->albumentations)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.13/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: webencodings in ./.venv/lib/python3.13/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Collecting text-unidecode>=1.3 (from python-slugify->kaggle)\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.13/site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.20.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m620.8/620.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m  \u001b[33m0:05:06\u001b[0mm0:00:01\u001b[0m00:08\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.76.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m  \u001b[33m0:00:32\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.2.6-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading matplotlib-3.10.7-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading scikit_learn-1.7.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kaggle-1.8.2-py3-none-any.whl (256 kB)\n",
      "Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
      "Downloading albucore-0.0.24-py3-none-any.whl (15 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Downloading black-25.11.0-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading contourpy-1.3.3-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Downloading fonttools-4.60.1-cp313-cp313-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.15.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Downloading keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Downloading mypy-1.18.2-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m  \u001b[33m0:00:29\u001b[0mm0:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading pytokens-0.3.0-py3-none-any.whl (12 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.16.3-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m  \u001b[33m0:00:17\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading simsimd-6.5.3-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading stringzilla-4.3.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (653 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m653.1/653.1 kB\u001b[0m \u001b[31m649.2 kB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading wrapt-2.0.1-cp313-cp313-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Downloading kagglesdk-0.1.13-py3-none-any.whl (159 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (414 kB)\n",
      "Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Downloading types_tqdm-4.67.0.20250809-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: text-unidecode, simsimd, pytz, namex, libclang, flatbuffers, wrapt, wheel, werkzeug, typing-inspection, types-requests, tqdm, threadpoolctl, termcolor, tensorboard-data-server, stringzilla, pytokens, python-slugify, pyparsing, pydantic-core, protobuf, pillow, pathspec, optree, opt_einsum, numpy, mypy-extensions, mdurl, markdown, kiwisolver, joblib, grpcio, google_pasta, gast, fonttools, cycler, click, annotated-types, absl-py, types-tqdm, tensorboard, scipy, pydantic, pandas, opencv-python-headless, opencv-python, mypy, ml_dtypes, markdown-it-py, kagglesdk, h5py, contourpy, black, astunparse, scikit-learn, rich, matplotlib, kaggle, albucore, seaborn, keras, albumentations, tensorflow\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63/63\u001b[0m [tensorflow]nsorflow]bumentations]eadless]\n",
      "\u001b[1A\u001b[2KSuccessfully installed absl-py-2.3.1 albucore-0.0.24 albumentations-2.0.8 annotated-types-0.7.0 astunparse-1.6.3 black-25.11.0 click-8.3.1 contourpy-1.3.3 cycler-0.12.1 flatbuffers-25.9.23 fonttools-4.60.1 gast-0.6.0 google_pasta-0.2.0 grpcio-1.76.0 h5py-3.15.1 joblib-1.5.2 kaggle-1.8.2 kagglesdk-0.1.13 keras-3.12.0 kiwisolver-1.4.9 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 matplotlib-3.10.7 mdurl-0.1.2 ml_dtypes-0.5.4 mypy-1.18.2 mypy-extensions-1.1.0 namex-0.1.0 numpy-2.2.6 opencv-python-4.12.0.88 opencv-python-headless-4.12.0.88 opt_einsum-3.4.0 optree-0.18.0 pandas-2.3.3 pathspec-0.12.1 pillow-12.0.0 protobuf-6.33.1 pydantic-2.12.5 pydantic-core-2.41.5 pyparsing-3.2.5 python-slugify-8.0.4 pytokens-0.3.0 pytz-2025.2 rich-14.2.0 scikit-learn-1.7.2 scipy-1.16.3 seaborn-0.13.2 simsimd-6.5.3 stringzilla-4.3.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 text-unidecode-1.3 threadpoolctl-3.6.0 tqdm-4.67.1 types-requests-2.32.4.20250913 types-tqdm-4.67.0.20250809 typing-inspection-0.4.2 werkzeug-3.1.3 wheel-0.45.1 wrapt-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "!pip install tensorflow opencv-python matplotlib numpy pandas seaborn scikit-learn pillow kaggle albumentations tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7e082b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-27 21:58:58.506673: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-27 21:58:58.547288: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-27 21:58:59.548925: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Image Processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep Learning - TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Sklearn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "251db36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.20.0\n",
      "Keras Version: 3.12.0\n",
      "NumPy Version: 2.2.6\n",
      "OpenCV Version: 4.12.0\n",
      "\n",
      "==================================================\n",
      "\n",
      "âš ï¸  No GPU detected. Training will use CPU (slower).\n",
      "   Consider using Google Colab for free GPU access.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764277145.754725   73343 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1764277145.760652   73343 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow version and GPU availability\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"OpenCV Version: {cv2.__version__}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check for GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"\\nğŸš€ GPU Available: {len(gpus)} GPU(s) detected\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"   - {gpu.name}\")\n",
    "    # Enable memory growth to prevent TensorFlow from allocating all GPU memory\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "else:\n",
    "    print(\"\\nâš ï¸  No GPU detected. Training will use CPU (slower).\")\n",
    "    print(\"   Consider using Google Colab for free GPU access.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a779e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    \"\"\"Set random seeds for reproducibility across all libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # For deterministic operations (may slow down training)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    print(f\"âœ… Random seed set to {seed} for reproducibility\")\n",
    "\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b7a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Configuration\n",
    "class Config:\n",
    "    \"\"\"Configuration class for all hyperparameters and settings.\"\"\"\n",
    "    \n",
    "    # Paths - Using the archive/Dataset folder with actual vehicle images\n",
    "    BASE_DIR = Path('.')\n",
    "    DATA_DIR = BASE_DIR / 'archive' / 'Dataset'  # Path to the extracted dataset\n",
    "    MODELS_DIR = BASE_DIR / 'models'\n",
    "    RESULTS_DIR = BASE_DIR / 'results'\n",
    "    OUTPUT_DIR = BASE_DIR / 'outputs'\n",
    "    MODEL_DIR = BASE_DIR / 'models'\n",
    "    \n",
    "    # Image settings\n",
    "    IMG_HEIGHT = 224\n",
    "    IMG_WIDTH = 224\n",
    "    IMG_CHANNELS = 3\n",
    "    IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n",
    "    \n",
    "    # Vehicle classes (matching folder names in archive/Dataset)\n",
    "    # Note: folder names are Bus, Car, Truck, motorcycle (case-sensitive)\n",
    "    CLASSES = ['bus', 'car', 'motorcycle', 'truck']\n",
    "    NUM_CLASSES = len(CLASSES)\n",
    "    \n",
    "    # Mapping from folder names to class names\n",
    "    FOLDER_TO_CLASS = {\n",
    "        'Bus': 'bus',\n",
    "        'Car': 'car',\n",
    "        'motorcycle': 'motorcycle',\n",
    "        'Truck': 'truck'\n",
    "    }\n",
    "    \n",
    "    # Training settings\n",
    "    BATCH_SIZE = 32\n",
    "    EPOCHS = 50\n",
    "    LEARNING_RATE = 0.001\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    TEST_SPLIT = 0.1\n",
    "    \n",
    "    # Data augmentation\n",
    "    AUGMENTATION = True\n",
    "    \n",
    "    # Early stopping\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    \n",
    "    # Random seed\n",
    "    SEED = 42\n",
    "\n",
    "# Create necessary directories\n",
    "for dir_path in [Config.MODELS_DIR, Config.RESULTS_DIR, Config.OUTPUT_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"ğŸ“ Directory ready: {dir_path}\")\n",
    "\n",
    "# Verify dataset exists\n",
    "if Config.DATA_DIR.exists():\n",
    "    print(f\"\\nâœ… Dataset found at: {Config.DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Dataset NOT found at: {Config.DATA_DIR}\")\n",
    "    print(\"   Please extract the dataset to the archive/Dataset folder\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ“‹ Configuration Summary:\")\n",
    "print(f\"   Image Size: {Config.IMG_HEIGHT}x{Config.IMG_WIDTH}x{Config.IMG_CHANNELS}\")\n",
    "print(f\"   Classes: {Config.CLASSES}\")\n",
    "print(f\"   Batch Size: {Config.BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {Config.EPOCHS}\")\n",
    "print(f\"   Learning Rate: {Config.LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for visualization\n",
    "def plot_image_grid(images, titles=None, figsize=(15, 10), rows=2, cols=4):\n",
    "    \"\"\"Plot a grid of images.\"\"\"\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (img, ax) in enumerate(zip(images, axes)):\n",
    "        if img.max() <= 1.0:\n",
    "            img = (img * 255).astype(np.uint8)\n",
    "        ax.imshow(img)\n",
    "        if titles and idx < len(titles):\n",
    "            ax.set_title(titles[idx], fontsize=10)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(images), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def draw_bounding_box(image, bbox, label, confidence, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    Draw bounding box with label and confidence on image.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (numpy array)\n",
    "        bbox: Bounding box coordinates (x_min, y_min, x_max, y_max)\n",
    "        label: Class label string\n",
    "        confidence: Confidence score (0-1)\n",
    "        color: BGR color tuple\n",
    "    \n",
    "    Returns:\n",
    "        Image with bounding box drawn\n",
    "    \"\"\"\n",
    "    img = image.copy()\n",
    "    x_min, y_min, x_max, y_max = [int(coord) for coord in bbox]\n",
    "    \n",
    "    # Draw rectangle\n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "    \n",
    "    # Prepare label text\n",
    "    text = f\"{label}: {confidence*100:.1f}%\"\n",
    "    \n",
    "    # Get text size for background rectangle\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.6\n",
    "    thickness = 2\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "    \n",
    "    # Draw background rectangle for text\n",
    "    cv2.rectangle(img, \n",
    "                  (x_min, y_min - text_height - 10), \n",
    "                  (x_min + text_width + 5, y_min), \n",
    "                  color, -1)\n",
    "    \n",
    "    # Draw text\n",
    "    cv2.putText(img, text, (x_min + 2, y_min - 5), \n",
    "                font, font_scale, (255, 255, 255), thickness)\n",
    "    \n",
    "    return img\n",
    "\n",
    "print(\"âœ… Utility functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd74acf",
   "metadata": {},
   "source": [
    "### Step 1 Complete! âœ…\n",
    "\n",
    "We have successfully:\n",
    "1. Imported all necessary libraries\n",
    "2. Verified TensorFlow installation and GPU availability\n",
    "3. Set random seeds for reproducibility\n",
    "4. Created configuration class with all hyperparameters\n",
    "5. Set up directory structure\n",
    "6. Defined utility functions for visualization\n",
    "\n",
    "**Next Step:** Step 2 - Dataset Download and Preparation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe740a",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Loading and Preparation\n",
    "\n",
    "The dataset is already extracted in the `archive/Dataset` folder with the following structure:\n",
    "\n",
    "```\n",
    "archive/\n",
    "â””â”€â”€ Dataset/\n",
    "    â”œâ”€â”€ Bus/         (100 images)\n",
    "    â”œâ”€â”€ Car/         (100 images)\n",
    "    â”œâ”€â”€ Truck/       (100 images)\n",
    "    â””â”€â”€ motorcycle/  (100 images)\n",
    "```\n",
    "\n",
    "**Total: 400 vehicle images across 4 classes**\n",
    "\n",
    "In this step, we will:\n",
    "1. Load images from the dataset folder\n",
    "2. Split the data into train, validation, and test sets\n",
    "3. Explore the dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb35dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Explore the Dataset from archive/Dataset\n",
    "def load_dataset_info():\n",
    "    \"\"\"\n",
    "    Load dataset information from the archive/Dataset folder.\n",
    "    Returns a dictionary with image paths and labels.\n",
    "    \"\"\"\n",
    "    dataset_info = {\n",
    "        'image_paths': [],\n",
    "        'labels': [],\n",
    "        'class_counts': {}\n",
    "    }\n",
    "    \n",
    "    # Folder names in the dataset (case-sensitive)\n",
    "    folder_names = ['Bus', 'Car', 'Truck', 'motorcycle']\n",
    "    \n",
    "    print(\"ğŸ“‚ Loading dataset from:\", Config.DATA_DIR)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for folder_name in folder_names:\n",
    "        folder_path = Config.DATA_DIR / folder_name\n",
    "        \n",
    "        if folder_path.exists():\n",
    "            # Get the class name (lowercase for consistency)\n",
    "            class_name = Config.FOLDER_TO_CLASS.get(folder_name, folder_name.lower())\n",
    "            \n",
    "            # Find all images in the folder\n",
    "            image_files = list(folder_path.glob('*.jpg')) + \\\n",
    "                         list(folder_path.glob('*.jpeg')) + \\\n",
    "                         list(folder_path.glob('*.png'))\n",
    "            \n",
    "            # Add to dataset info\n",
    "            for img_path in image_files:\n",
    "                dataset_info['image_paths'].append(str(img_path))\n",
    "                dataset_info['labels'].append(class_name)\n",
    "            \n",
    "            # Count images per class\n",
    "            dataset_info['class_counts'][class_name] = len(image_files)\n",
    "            \n",
    "            print(f\"   âœ… {folder_name:12s} â†’ {class_name:12s}: {len(image_files):3d} images\")\n",
    "        else:\n",
    "            print(f\"   âŒ {folder_name:12s} â†’ Folder not found!\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ğŸ“Š Total images: {len(dataset_info['image_paths'])}\")\n",
    "    \n",
    "    return dataset_info\n",
    "\n",
    "# Load dataset info\n",
    "dataset_info = load_dataset_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12974987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Class Distribution\n",
    "def plot_class_distribution(dataset_info):\n",
    "    \"\"\"Plot the distribution of images across classes.\"\"\"\n",
    "    classes = list(dataset_info['class_counts'].keys())\n",
    "    counts = list(dataset_info['class_counts'].values())\n",
    "    \n",
    "    # Create bar plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "    bars = ax.bar(classes, counts, color=colors, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, count in zip(bars, counts):\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{count}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Vehicle Class', fontsize=12)\n",
    "    ax.set_ylabel('Number of Images', fontsize=12)\n",
    "    ax.set_title('Dataset Class Distribution', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim(0, max(counts) * 1.15)\n",
    "    \n",
    "    # Add total count\n",
    "    total = sum(counts)\n",
    "    ax.text(0.98, 0.95, f'Total: {total} images', transform=ax.transAxes,\n",
    "            fontsize=11, verticalalignment='top', horizontalalignment='right',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.RESULTS_DIR / 'class_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nğŸ“Š Class Distribution Summary:\")\n",
    "    for cls, count in zip(classes, counts):\n",
    "        percentage = (count / total) * 100\n",
    "        print(f\"   {cls.capitalize():12s}: {count:4d} images ({percentage:.1f}%)\")\n",
    "\n",
    "plot_class_distribution(dataset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a50eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sample Images from Each Class\n",
    "def display_sample_images(num_samples=4):\n",
    "    \"\"\"Display random sample images from each vehicle class.\"\"\"\n",
    "    \n",
    "    # Folder names in the dataset (case-sensitive)\n",
    "    folder_names = ['Bus', 'Car', 'Truck', 'motorcycle']\n",
    "    \n",
    "    fig, axes = plt.subplots(len(folder_names), num_samples, figsize=(4*num_samples, 4*len(folder_names)))\n",
    "    \n",
    "    for row, folder_name in enumerate(folder_names):\n",
    "        class_dir = Config.DATA_DIR / folder_name\n",
    "        \n",
    "        if class_dir.exists():\n",
    "            # Get image files\n",
    "            image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png')) + list(class_dir.glob('*.jpeg'))\n",
    "            \n",
    "            if image_files:\n",
    "                # Randomly select samples\n",
    "                samples = random.sample(image_files, min(num_samples, len(image_files)))\n",
    "                \n",
    "                for col, img_path in enumerate(samples):\n",
    "                    # Load and display image\n",
    "                    img = cv2.imread(str(img_path))\n",
    "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    ax = axes[row, col] if num_samples > 1 else axes[row]\n",
    "                    ax.imshow(img)\n",
    "                    ax.set_title(f'{folder_name}\\n{img_path.name[:20]}...', fontsize=10)\n",
    "                    ax.axis('off')\n",
    "            else:\n",
    "                for col in range(num_samples):\n",
    "                    ax = axes[row, col] if num_samples > 1 else axes[row]\n",
    "                    ax.text(0.5, 0.5, 'No images found', ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.axis('off')\n",
    "        else:\n",
    "            for col in range(num_samples):\n",
    "                ax = axes[row, col] if num_samples > 1 else axes[row]\n",
    "                ax.text(0.5, 0.5, 'Directory not found', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Images from Each Vehicle Class', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.RESULTS_DIR / 'sample_images.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "display_sample_images(num_samples=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052822a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Image Properties (size, aspect ratio, color distribution)\n",
    "def analyze_image_properties():\n",
    "    \"\"\"Analyze properties of images in the dataset.\"\"\"\n",
    "    print(\"ğŸ” Analyzing image properties...\")\n",
    "    \n",
    "    heights = []\n",
    "    widths = []\n",
    "    aspects = []\n",
    "    \n",
    "    # Folder names in the dataset\n",
    "    folder_names = ['Bus', 'Car', 'Truck', 'motorcycle']\n",
    "    \n",
    "    for folder_name in folder_names:\n",
    "        class_dir = Config.DATA_DIR / folder_name\n",
    "        if class_dir.exists():\n",
    "            image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.png')) + list(class_dir.glob('*.jpeg'))\n",
    "            \n",
    "            for img_path in image_files[:50]:  # Sample up to 50 images per class\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    w, h = img.size\n",
    "                    heights.append(h)\n",
    "                    widths.append(w)\n",
    "                    aspects.append(w / h)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    \n",
    "    if heights:\n",
    "        print(f\"\\nğŸ“ Image Dimensions:\")\n",
    "        print(f\"   Height - Min: {min(heights)}, Max: {max(heights)}, Mean: {np.mean(heights):.1f}\")\n",
    "        print(f\"   Width  - Min: {min(widths)}, Max: {max(widths)}, Mean: {np.mean(widths):.1f}\")\n",
    "        print(f\"   Aspect Ratio - Min: {min(aspects):.2f}, Max: {max(aspects):.2f}, Mean: {np.mean(aspects):.2f}\")\n",
    "        \n",
    "        # Plot distributions\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "        \n",
    "        axes[0].hist(heights, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "        axes[0].set_xlabel('Height (pixels)')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].set_title('Image Height Distribution')\n",
    "        axes[0].axvline(np.mean(heights), color='red', linestyle='--', label=f'Mean: {np.mean(heights):.0f}')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        axes[1].hist(widths, bins=30, color='orange', edgecolor='black', alpha=0.7)\n",
    "        axes[1].set_xlabel('Width (pixels)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Image Width Distribution')\n",
    "        axes[1].axvline(np.mean(widths), color='red', linestyle='--', label=f'Mean: {np.mean(widths):.0f}')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        axes[2].hist(aspects, bins=30, color='green', edgecolor='black', alpha=0.7)\n",
    "        axes[2].set_xlabel('Aspect Ratio (W/H)')\n",
    "        axes[2].set_ylabel('Frequency')\n",
    "        axes[2].set_title('Aspect Ratio Distribution')\n",
    "        axes[2].axvline(np.mean(aspects), color='red', linestyle='--', label=f'Mean: {np.mean(aspects):.2f}')\n",
    "        axes[2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Config.RESULTS_DIR / 'image_properties.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"âš ï¸  No images found to analyze.\")\n",
    "\n",
    "analyze_image_properties()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdeacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load All Images and Create Train/Validation/Test Splits\n",
    "def load_and_preprocess_image(image_path, target_size=(Config.IMG_HEIGHT, Config.IMG_WIDTH)):\n",
    "    \"\"\"Load and preprocess a single image.\"\"\"\n",
    "    img = cv2.imread(str(image_path))\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return img\n",
    "\n",
    "def load_full_dataset():\n",
    "    \"\"\"\n",
    "    Load all images from archive/Dataset folder.\n",
    "    Returns arrays of images and labels.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Folder names in the dataset (case-sensitive)\n",
    "    folder_names = ['Bus', 'Car', 'Truck', 'motorcycle']\n",
    "    \n",
    "    print(\"ğŸ“‚ Loading all images from dataset...\")\n",
    "    \n",
    "    for folder_name in folder_names:\n",
    "        class_dir = Config.DATA_DIR / folder_name\n",
    "        class_name = Config.FOLDER_TO_CLASS.get(folder_name, folder_name.lower())\n",
    "        class_idx = Config.CLASSES.index(class_name)\n",
    "        \n",
    "        if class_dir.exists():\n",
    "            image_files = list(class_dir.glob('*.jpg')) + \\\n",
    "                         list(class_dir.glob('*.jpeg')) + \\\n",
    "                         list(class_dir.glob('*.png'))\n",
    "            \n",
    "            for img_path in tqdm(image_files, desc=f\"Loading {folder_name}\"):\n",
    "                img = load_and_preprocess_image(img_path)\n",
    "                if img is not None:\n",
    "                    images.append(img)\n",
    "                    labels.append(class_idx)\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    print(f\"\\nâœ… Loaded {len(images)} images\")\n",
    "    print(f\"   Image shape: {images[0].shape}\")\n",
    "    print(f\"   Labels shape: {labels.shape}\")\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Load the full dataset\n",
    "X, y = load_full_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deb1b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset into Train, Validation, and Test Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split: separate test set (10%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=Config.TEST_SPLIT, \n",
    "    random_state=Config.SEED, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Second split: separate validation set (20% of remaining = ~18% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=Config.VALIDATION_SPLIT, \n",
    "    random_state=Config.SEED, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š Dataset Split Summary:\")\n",
    "print(f\"   Training set:   {len(X_train):4d} images ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Validation set: {len(X_val):4d} images ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Test set:       {len(X_test):4d} images ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Total:          {len(X):4d} images\")\n",
    "\n",
    "# Verify class distribution in each split\n",
    "print(\"\\nğŸ“ˆ Class distribution per split:\")\n",
    "for split_name, split_y in [('Train', y_train), ('Val', y_val), ('Test', y_test)]:\n",
    "    unique, counts = np.unique(split_y, return_counts=True)\n",
    "    class_dist = {Config.CLASSES[i]: c for i, c in zip(unique, counts)}\n",
    "    print(f\"   {split_name:5s}: {class_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb66bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Labels to One-Hot Encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_onehot = to_categorical(y_train, num_classes=Config.NUM_CLASSES)\n",
    "y_val_onehot = to_categorical(y_val, num_classes=Config.NUM_CLASSES)\n",
    "y_test_onehot = to_categorical(y_test, num_classes=Config.NUM_CLASSES)\n",
    "\n",
    "print(\"âœ… Labels converted to one-hot encoding\")\n",
    "print(f\"   y_train shape: {y_train_onehot.shape}\")\n",
    "print(f\"   y_val shape:   {y_val_onehot.shape}\")\n",
    "print(f\"   y_test shape:  {y_test_onehot.shape}\")\n",
    "print(f\"\\n   Example one-hot label: {y_train_onehot[0]} â†’ {Config.CLASSES[y_train[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94720e68",
   "metadata": {},
   "source": [
    "### Step 2 Complete! âœ…\n",
    "\n",
    "We have successfully:\n",
    "1. Loaded 400 vehicle images from `archive/Dataset/` folder\n",
    "2. Explored the dataset with sample images and statistics\n",
    "3. Analyzed image properties (dimensions, aspect ratios)\n",
    "4. Split data into Train (70%), Validation (20%), and Test (10%) sets\n",
    "5. Converted labels to one-hot encoding\n",
    "\n",
    "**Dataset Summary:**\n",
    "- ğŸšŒ Bus: 100 images\n",
    "- ğŸš— Car: 100 images\n",
    "- ğŸï¸ Motorcycle: 100 images\n",
    "- ğŸšš Truck: 100 images\n",
    "- **Total: 400 images**\n",
    "\n",
    "**Next Step:** Step 3 - Data Preprocessing and Augmentation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec1170",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing and Augmentation\n",
    "\n",
    "Data augmentation is crucial for training robust CNN models, especially when working with limited data. It helps:\n",
    "- **Prevent overfitting** by creating variations of training images\n",
    "- **Improve generalization** to new, unseen images\n",
    "- **Balance class distributions** by generating more samples\n",
    "\n",
    "### Augmentation Techniques we'll use:\n",
    "1. **Geometric transformations**: rotation, flipping, shifting, zooming\n",
    "2. **Color/brightness adjustments**: brightness, contrast, saturation\n",
    "3. **Noise injection**: random noise for robustness\n",
    "4. **Normalization**: standardize pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e96ebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation using Keras ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Training data augmentation - apply various transformations\n",
    "train_datagen = ImageDataGenerator(\n",
    "    # Geometric transformations\n",
    "    rotation_range=15,           # Random rotation up to 15 degrees\n",
    "    width_shift_range=0.1,       # Horizontal shift up to 10%\n",
    "    height_shift_range=0.1,      # Vertical shift up to 10%\n",
    "    shear_range=0.1,             # Shear transformation\n",
    "    zoom_range=0.15,             # Random zoom up to 15%\n",
    "    horizontal_flip=True,        # Random horizontal flip\n",
    "    \n",
    "    # Color transformations\n",
    "    brightness_range=[0.8, 1.2], # Brightness variation\n",
    "    channel_shift_range=20,      # Random channel shifts\n",
    "    \n",
    "    # Fill mode for new pixels\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Validation/Test data - only rescaling, no augmentation\n",
    "val_datagen = ImageDataGenerator()  # No augmentation for validation\n",
    "\n",
    "print(\"âœ… Data augmentation generators created!\")\n",
    "print(\"\\nğŸ“‹ Training Augmentations:\")\n",
    "print(\"   â€¢ Rotation: Â±15Â°\")\n",
    "print(\"   â€¢ Width/Height shift: Â±10%\")\n",
    "print(\"   â€¢ Shear: 10%\")\n",
    "print(\"   â€¢ Zoom: Â±15%\")\n",
    "print(\"   â€¢ Horizontal flip: Yes\")\n",
    "print(\"   â€¢ Brightness: 80%-120%\")\n",
    "print(\"   â€¢ Channel shift: Â±20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a475b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Data Augmentation Effects\n",
    "def visualize_augmentation(image, num_augmentations=8):\n",
    "    \"\"\"\n",
    "    Visualize the effect of data augmentation on a single image.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original', fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Generate augmented versions\n",
    "    # Expand dims for the generator\n",
    "    img_array = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # Create iterator\n",
    "    aug_iter = train_datagen.flow(img_array, batch_size=1)\n",
    "    \n",
    "    for i in range(1, num_augmentations):\n",
    "        aug_img = next(aug_iter)[0]\n",
    "        # Clip values to valid range\n",
    "        aug_img = np.clip(aug_img, 0, 1)\n",
    "        axes[i].imshow(aug_img)\n",
    "        axes[i].set_title(f'Augmented {i}', fontsize=12)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle('Data Augmentation Visualization', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.RESULTS_DIR / 'augmentation_examples.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize augmentation on a sample image from training set\n",
    "if len(X_train) > 0:\n",
    "    sample_idx = random.randint(0, len(X_train) - 1)\n",
    "    sample_image = X_train[sample_idx]\n",
    "    sample_class = Config.CLASSES[y_train[sample_idx]]\n",
    "    print(f\"ğŸ–¼ï¸  Sample image class: {sample_class}\")\n",
    "    visualize_augmentation(sample_image)\n",
    "else:\n",
    "    print(\"âš ï¸  No training images available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Augmentation Functions for more control\n",
    "def apply_random_noise(image, noise_factor=0.05):\n",
    "    \"\"\"Add random Gaussian noise to image.\"\"\"\n",
    "    noise = np.random.normal(0, noise_factor, image.shape)\n",
    "    noisy_image = np.clip(image + noise, 0, 1)\n",
    "    return noisy_image.astype(np.float32)\n",
    "\n",
    "def apply_random_blur(image, kernel_size=3):\n",
    "    \"\"\"Apply random Gaussian blur.\"\"\"\n",
    "    if random.random() > 0.5:\n",
    "        # Convert to uint8 for cv2\n",
    "        img_uint8 = (image * 255).astype(np.uint8)\n",
    "        blurred = cv2.GaussianBlur(img_uint8, (kernel_size, kernel_size), 0)\n",
    "        return blurred.astype(np.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "def apply_random_contrast(image, factor_range=(0.8, 1.2)):\n",
    "    \"\"\"Adjust image contrast randomly.\"\"\"\n",
    "    factor = random.uniform(*factor_range)\n",
    "    mean = np.mean(image, axis=(0, 1), keepdims=True)\n",
    "    adjusted = (image - mean) * factor + mean\n",
    "    return np.clip(adjusted, 0, 1).astype(np.float32)\n",
    "\n",
    "def apply_random_saturation(image, factor_range=(0.8, 1.2)):\n",
    "    \"\"\"Adjust image saturation randomly.\"\"\"\n",
    "    # Convert to HSV\n",
    "    img_uint8 = (image * 255).astype(np.uint8)\n",
    "    hsv = cv2.cvtColor(img_uint8, cv2.COLOR_RGB2HSV).astype(np.float32)\n",
    "    \n",
    "    # Adjust saturation\n",
    "    factor = random.uniform(*factor_range)\n",
    "    hsv[:, :, 1] = np.clip(hsv[:, :, 1] * factor, 0, 255)\n",
    "    \n",
    "    # Convert back to RGB\n",
    "    hsv = hsv.astype(np.uint8)\n",
    "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    return rgb.astype(np.float32) / 255.0\n",
    "\n",
    "def augment_image(image):\n",
    "    \"\"\"Apply a random combination of augmentations.\"\"\"\n",
    "    # Apply each augmentation with 50% probability\n",
    "    if random.random() > 0.5:\n",
    "        image = apply_random_noise(image)\n",
    "    if random.random() > 0.5:\n",
    "        image = apply_random_blur(image)\n",
    "    if random.random() > 0.5:\n",
    "        image = apply_random_contrast(image)\n",
    "    if random.random() > 0.5:\n",
    "        image = apply_random_saturation(image)\n",
    "    return image\n",
    "\n",
    "print(\"âœ… Custom augmentation functions defined!\")\n",
    "print(\"   â€¢ Random noise injection\")\n",
    "print(\"   â€¢ Random Gaussian blur\")\n",
    "print(\"   â€¢ Random contrast adjustment\")\n",
    "print(\"   â€¢ Random saturation adjustment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Custom Augmentations\n",
    "def visualize_custom_augmentations(image):\n",
    "    \"\"\"Show effects of each custom augmentation.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    augmentations = [\n",
    "        ('Original', image),\n",
    "        ('+ Noise', apply_random_noise(image, 0.08)),\n",
    "        ('+ Blur', apply_random_blur(image, 5)),\n",
    "        ('+ Contrast', apply_random_contrast(image, (0.6, 1.4))),\n",
    "        ('+ Saturation', apply_random_saturation(image, (0.5, 1.5))),\n",
    "        ('Combined', augment_image(image.copy()))\n",
    "    ]\n",
    "    \n",
    "    for ax, (title, img) in zip(axes, augmentations):\n",
    "        ax.imshow(np.clip(img, 0, 1))\n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Custom Augmentation Effects', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.RESULTS_DIR / 'custom_augmentations.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Show custom augmentations\n",
    "if len(X_train) > 0:\n",
    "    sample_image = X_train[random.randint(0, len(X_train) - 1)]\n",
    "    visualize_custom_augmentations(sample_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization - Compute dataset statistics\n",
    "def compute_dataset_statistics(images):\n",
    "    \"\"\"Compute mean and std for normalization.\"\"\"\n",
    "    mean = np.mean(images, axis=(0, 1, 2))\n",
    "    std = np.std(images, axis=(0, 1, 2))\n",
    "    return mean, std\n",
    "\n",
    "# Compute training set statistics for normalization\n",
    "train_mean, train_std = compute_dataset_statistics(X_train)\n",
    "\n",
    "print(\"ğŸ“Š Training Set Statistics (per channel):\")\n",
    "print(f\"   Mean (RGB): [{train_mean[0]:.4f}, {train_mean[1]:.4f}, {train_mean[2]:.4f}]\")\n",
    "print(f\"   Std  (RGB): [{train_std[0]:.4f}, {train_std[1]:.4f}, {train_std[2]:.4f}]\")\n",
    "\n",
    "# Normalization function\n",
    "def normalize_images(images, mean, std):\n",
    "    \"\"\"Normalize images using mean and std.\"\"\"\n",
    "    return (images - mean) / (std + 1e-7)\n",
    "\n",
    "def denormalize_images(images, mean, std):\n",
    "    \"\"\"Denormalize images for visualization.\"\"\"\n",
    "    return images * std + mean\n",
    "\n",
    "# Note: We'll use [0,1] normalization for simplicity\n",
    "# The images are already normalized to [0,1] during loading\n",
    "print(\"\\nâœ… Using [0, 1] normalization (already applied during loading)\")\n",
    "print(\"   For ImageNet-style normalization, use the mean/std values above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cd8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorFlow Data Pipeline for efficient training\n",
    "def create_tf_dataset(images, labels, batch_size, augment=False, shuffle=True):\n",
    "    \"\"\"\n",
    "    Create an efficient tf.data.Dataset pipeline.\n",
    "    \n",
    "    Args:\n",
    "        images: NumPy array of images\n",
    "        labels: NumPy array of one-hot labels\n",
    "        batch_size: Batch size for training\n",
    "        augment: Whether to apply augmentation\n",
    "        shuffle: Whether to shuffle the data\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset object\n",
    "    \"\"\"\n",
    "    # Create dataset from tensors\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "    \n",
    "    # Shuffle if requested\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(images), seed=Config.SEED)\n",
    "    \n",
    "    # Define augmentation function for tf.data\n",
    "    def tf_augment(image, label):\n",
    "        \"\"\"Apply augmentations using TensorFlow operations.\"\"\"\n",
    "        # Random flip\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        \n",
    "        # Random brightness\n",
    "        image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "        \n",
    "        # Random contrast\n",
    "        image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "        \n",
    "        # Random saturation\n",
    "        image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "        \n",
    "        # Random hue (small changes)\n",
    "        image = tf.image.random_hue(image, max_delta=0.1)\n",
    "        \n",
    "        # Clip to valid range\n",
    "        image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    # Apply augmentation if requested\n",
    "    if augment:\n",
    "        dataset = dataset.map(tf_augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Batch and prefetch for performance\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "print(\"âœ… TensorFlow data pipeline function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d5f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data pipelines for training, validation, and testing\n",
    "print(\"ğŸ”„ Creating data pipelines...\")\n",
    "\n",
    "# Training dataset with augmentation\n",
    "train_dataset = create_tf_dataset(\n",
    "    X_train, y_train_onehot, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    augment=True, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Validation dataset without augmentation\n",
    "val_dataset = create_tf_dataset(\n",
    "    X_val, y_val_onehot, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    augment=False, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Test dataset without augmentation\n",
    "test_dataset = create_tf_dataset(\n",
    "    X_test, y_test_onehot, \n",
    "    batch_size=Config.BATCH_SIZE, \n",
    "    augment=False, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(\"âœ… Data pipelines created!\")\n",
    "print(f\"\\nğŸ“Š Pipeline Information:\")\n",
    "print(f\"   Training batches: {len(X_train) // Config.BATCH_SIZE + 1}\")\n",
    "print(f\"   Validation batches: {len(X_val) // Config.BATCH_SIZE + 1}\")\n",
    "print(f\"   Test batches: {len(X_test) // Config.BATCH_SIZE + 1}\")\n",
    "\n",
    "# Verify pipeline output shape\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(f\"\\n   Batch image shape: {images.shape}\")\n",
    "    print(f\"   Batch label shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdea210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch from the augmented training pipeline\n",
    "def visualize_batch(dataset, num_images=8):\n",
    "    \"\"\"Visualize a batch of images from the dataset.\"\"\"\n",
    "    # Get one batch\n",
    "    for images, labels in dataset.take(1):\n",
    "        images = images.numpy()\n",
    "        labels = labels.numpy()\n",
    "        \n",
    "        # Convert one-hot to class indices\n",
    "        class_indices = np.argmax(labels, axis=1)\n",
    "        \n",
    "        # Plot images\n",
    "        rows = 2\n",
    "        cols = num_images // 2\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i in range(min(num_images, len(images))):\n",
    "            img = np.clip(images[i], 0, 1)\n",
    "            class_name = Config.CLASSES[class_indices[i]]\n",
    "            \n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f'{class_name.capitalize()}', fontsize=12)\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Hide remaining axes\n",
    "        for i in range(num_images, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('Augmented Training Batch', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(Config.RESULTS_DIR / 'augmented_batch.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "print(\"ğŸ–¼ï¸  Visualizing an augmented training batch...\")\n",
    "visualize_batch(train_dataset, num_images=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798111ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class Weights for Handling Imbalanced Data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def compute_class_weights(labels):\n",
    "    \"\"\"\n",
    "    Compute class weights to handle imbalanced datasets.\n",
    "    Classes with fewer samples get higher weights.\n",
    "    \"\"\"\n",
    "    unique_classes = np.unique(labels)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=unique_classes,\n",
    "        y=labels\n",
    "    )\n",
    "    \n",
    "    # Convert to dictionary format for Keras\n",
    "    class_weight_dict = {i: w for i, w in enumerate(class_weights)}\n",
    "    return class_weight_dict\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weights(y_train)\n",
    "\n",
    "print(\"âš–ï¸  Class Weights (for handling imbalanced data):\")\n",
    "print(\"=\"*50)\n",
    "for class_idx, weight in class_weights.items():\n",
    "    class_name = Config.CLASSES[class_idx]\n",
    "    count = np.sum(y_train == class_idx)\n",
    "    print(f\"   {class_name.capitalize():12} | Weight: {weight:.4f} | Samples: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aff9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Preprocessing Pipeline\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“‹ STEP 3 SUMMARY: Data Preprocessing & Augmentation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nğŸ”„ Augmentation Techniques Applied (Training Only):\")\n",
    "print(\"   â”œâ”€â”€ Geometric Transformations:\")\n",
    "print(\"   â”‚   â”œâ”€â”€ Random horizontal flip\")\n",
    "print(\"   â”‚   â”œâ”€â”€ Random rotation (Â±15Â°)\")\n",
    "print(\"   â”‚   â”œâ”€â”€ Random shift (Â±10%)\")\n",
    "print(\"   â”‚   â””â”€â”€ Random zoom (Â±15%)\")\n",
    "print(\"   â””â”€â”€ Color Transformations:\")\n",
    "print(\"       â”œâ”€â”€ Random brightness (Â±20%)\")\n",
    "print(\"       â”œâ”€â”€ Random contrast (Â±20%)\")\n",
    "print(\"       â”œâ”€â”€ Random saturation (Â±20%)\")\n",
    "print(\"       â””â”€â”€ Random hue (Â±10%)\")\n",
    "\n",
    "print(\"\\nğŸ“Š Data Pipeline:\")\n",
    "print(f\"   â”œâ”€â”€ Training: {len(X_train)} images â†’ Augmented â†’ Batches of {Config.BATCH_SIZE}\")\n",
    "print(f\"   â”œâ”€â”€ Validation: {len(X_val)} images â†’ Batches of {Config.BATCH_SIZE}\")\n",
    "print(f\"   â””â”€â”€ Test: {len(X_test)} images â†’ Batches of {Config.BATCH_SIZE}\")\n",
    "\n",
    "print(\"\\nâš–ï¸  Class Balancing:\")\n",
    "print(\"   â””â”€â”€ Class weights computed for balanced training\")\n",
    "\n",
    "print(\"\\nâœ… Preprocessing complete! Ready for model building.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10b71c2",
   "metadata": {},
   "source": [
    "### Step 3 Complete! âœ…\n",
    "\n",
    "We have successfully implemented:\n",
    "\n",
    "1. **Keras ImageDataGenerator** - For traditional augmentation pipeline\n",
    "2. **Custom Augmentation Functions**:\n",
    "   - Random noise injection\n",
    "   - Random Gaussian blur\n",
    "   - Random contrast adjustment\n",
    "   - Random saturation adjustment\n",
    "\n",
    "3. **TensorFlow Data Pipeline** - Efficient tf.data.Dataset with:\n",
    "   - Random horizontal flip\n",
    "   - Random brightness/contrast/saturation/hue adjustments\n",
    "   - Automatic batching and prefetching\n",
    "\n",
    "4. **Data Normalization** - Computed dataset mean and std\n",
    "\n",
    "5. **Class Weights** - For handling imbalanced data during training\n",
    "\n",
    "6. **Visualization Functions** - To verify augmentation effects\n",
    "\n",
    "**Key Variables Ready for Training:**\n",
    "- `train_dataset` - Augmented training data pipeline\n",
    "- `val_dataset` - Validation data pipeline  \n",
    "- `test_dataset` - Test data pipeline\n",
    "- `class_weights` - For balanced training\n",
    "\n",
    "**Next Step:** Step 4 - CNN Model Architecture (Built from Scratch)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d34cb8",
   "metadata": {},
   "source": [
    "## Step 4: CNN Model Architecture (Built from Scratch)\n",
    "\n",
    "We'll build a custom Convolutional Neural Network from scratch for vehicle classification. The architecture will include:\n",
    "\n",
    "### Architecture Overview:\n",
    "1. **Input Layer**: 224Ã—224Ã—3 RGB images\n",
    "2. **Convolutional Blocks**: Feature extraction with Conv2D, BatchNorm, MaxPooling\n",
    "3. **Regularization**: Dropout layers to prevent overfitting\n",
    "4. **Classification Head**: Fully connected layers for vehicle type prediction\n",
    "\n",
    "### Key Design Principles:\n",
    "- **Progressive feature extraction**: Increasing filter depth (32 â†’ 64 â†’ 128 â†’ 256)\n",
    "- **Batch Normalization**: Faster training and better generalization\n",
    "- **Dropout**: Regularization to prevent overfitting\n",
    "- **Global Average Pooling**: Reduces parameters compared to Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39666b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import additional Keras layers for model building\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D,\n",
    "    Dense, Dropout, Flatten, BatchNormalization, Activation,\n",
    "    Input, Add, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "print(\"âœ… Keras layers imported for model building\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2822c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN Architecture - VehicleNet\n",
    "def build_vehicle_cnn(input_shape=Config.IMG_SHAPE, num_classes=Config.NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    Build a custom CNN architecture for vehicle classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - 5 Convolutional Blocks with increasing filters\n",
    "    - Batch Normalization after each Conv layer\n",
    "    - MaxPooling for downsampling\n",
    "    - Dropout for regularization\n",
    "    - Global Average Pooling\n",
    "    - Dense classification head\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input image shape (height, width, channels)\n",
    "        num_classes: Number of output classes\n",
    "    \n",
    "    Returns:\n",
    "        Keras Model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential(name='VehicleNet')\n",
    "    \n",
    "    # ============================================\n",
    "    # Block 1: Initial Feature Extraction\n",
    "    # Input: 224x224x3 -> Output: 112x112x32\n",
    "    # ============================================\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape, name='conv1_1'))\n",
    "    model.add(BatchNormalization(name='bn1_1'))\n",
    "    model.add(Activation('relu', name='relu1_1'))\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', name='conv1_2'))\n",
    "    model.add(BatchNormalization(name='bn1_2'))\n",
    "    model.add(Activation('relu', name='relu1_2'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='pool1'))\n",
    "    model.add(Dropout(0.25, name='dropout1'))\n",
    "    \n",
    "    # ============================================\n",
    "    # Block 2: 112x112x32 -> 56x56x64\n",
    "    # ============================================\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', name='conv2_1'))\n",
    "    model.add(BatchNormalization(name='bn2_1'))\n",
    "    model.add(Activation('relu', name='relu2_1'))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', name='conv2_2'))\n",
    "    model.add(BatchNormalization(name='bn2_2'))\n",
    "    model.add(Activation('relu', name='relu2_2'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='pool2'))\n",
    "    model.add(Dropout(0.25, name='dropout2'))\n",
    "    \n",
    "    # ============================================\n",
    "    # Block 3: 56x56x64 -> 28x28x128\n",
    "    # ============================================\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', name='conv3_1'))\n",
    "    model.add(BatchNormalization(name='bn3_1'))\n",
    "    model.add(Activation('relu', name='relu3_1'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', name='conv3_2'))\n",
    "    model.add(BatchNormalization(name='bn3_2'))\n",
    "    model.add(Activation('relu', name='relu3_2'))\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', name='conv3_3'))\n",
    "    model.add(BatchNormalization(name='bn3_3'))\n",
    "    model.add(Activation('relu', name='relu3_3'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='pool3'))\n",
    "    model.add(Dropout(0.3, name='dropout3'))\n",
    "    \n",
    "    # ============================================\n",
    "    # Block 4: 28x28x128 -> 14x14x256\n",
    "    # ============================================\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', name='conv4_1'))\n",
    "    model.add(BatchNormalization(name='bn4_1'))\n",
    "    model.add(Activation('relu', name='relu4_1'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', name='conv4_2'))\n",
    "    model.add(BatchNormalization(name='bn4_2'))\n",
    "    model.add(Activation('relu', name='relu4_2'))\n",
    "    model.add(Conv2D(256, (3, 3), padding='same', name='conv4_3'))\n",
    "    model.add(BatchNormalization(name='bn4_3'))\n",
    "    model.add(Activation('relu', name='relu4_3'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='pool4'))\n",
    "    model.add(Dropout(0.3, name='dropout4'))\n",
    "    \n",
    "    # ============================================\n",
    "    # Block 5: 14x14x256 -> 7x7x512\n",
    "    # ============================================\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', name='conv5_1'))\n",
    "    model.add(BatchNormalization(name='bn5_1'))\n",
    "    model.add(Activation('relu', name='relu5_1'))\n",
    "    model.add(Conv2D(512, (3, 3), padding='same', name='conv5_2'))\n",
    "    model.add(BatchNormalization(name='bn5_2'))\n",
    "    model.add(Activation('relu', name='relu5_2'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name='pool5'))\n",
    "    model.add(Dropout(0.4, name='dropout5'))\n",
    "    \n",
    "    # ============================================\n",
    "    # Classification Head\n",
    "    # ============================================\n",
    "    model.add(GlobalAveragePooling2D(name='global_avg_pool'))\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    model.add(Dense(512, name='fc1'))\n",
    "    model.add(BatchNormalization(name='bn_fc1'))\n",
    "    model.add(Activation('relu', name='relu_fc1'))\n",
    "    model.add(Dropout(0.5, name='dropout_fc1'))\n",
    "    \n",
    "    model.add(Dense(256, name='fc2'))\n",
    "    model.add(BatchNormalization(name='bn_fc2'))\n",
    "    model.add(Activation('relu', name='relu_fc2'))\n",
    "    model.add(Dropout(0.5, name='dropout_fc2'))\n",
    "    \n",
    "    # Output Layer\n",
    "    model.add(Dense(num_classes, activation='softmax', name='output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"âœ… VehicleNet architecture function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b718ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Lighter CNN Architecture for faster training\n",
    "def build_vehicle_cnn_lite(input_shape=Config.IMG_SHAPE, num_classes=Config.NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    A lighter version of VehicleNet for faster training.\n",
    "    Good for initial experimentation or limited compute resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential(name='VehicleNet_Lite')\n",
    "    \n",
    "    # Block 1: 224x224x3 -> 112x112x32\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Block 2: 112x112x32 -> 56x56x64\n",
    "    model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Block 3: 56x56x64 -> 28x28x128\n",
    "    model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Block 4: 28x28x128 -> 14x14x256\n",
    "    model.add(Conv2D(256, (3, 3), padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Classification Head\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"âœ… VehicleNet_Lite architecture function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092af90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Residual CNN Architecture with Skip Connections\n",
    "def build_vehicle_resnet(input_shape=Config.IMG_SHAPE, num_classes=Config.NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    A ResNet-inspired architecture with residual connections.\n",
    "    Better gradient flow and can train deeper networks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def residual_block(x, filters, kernel_size=3, stride=1, name_prefix=''):\n",
    "        \"\"\"A residual block with skip connection.\"\"\"\n",
    "        shortcut = x\n",
    "        \n",
    "        # First conv layer\n",
    "        x = Conv2D(filters, kernel_size, strides=stride, padding='same',\n",
    "                   name=f'{name_prefix}_conv1')(x)\n",
    "        x = BatchNormalization(name=f'{name_prefix}_bn1')(x)\n",
    "        x = Activation('relu', name=f'{name_prefix}_relu1')(x)\n",
    "        \n",
    "        # Second conv layer\n",
    "        x = Conv2D(filters, kernel_size, strides=1, padding='same',\n",
    "                   name=f'{name_prefix}_conv2')(x)\n",
    "        x = BatchNormalization(name=f'{name_prefix}_bn2')(x)\n",
    "        \n",
    "        # Adjust shortcut if dimensions don't match\n",
    "        if stride != 1 or shortcut.shape[-1] != filters:\n",
    "            shortcut = Conv2D(filters, 1, strides=stride, padding='same',\n",
    "                            name=f'{name_prefix}_shortcut')(shortcut)\n",
    "            shortcut = BatchNormalization(name=f'{name_prefix}_shortcut_bn')(shortcut)\n",
    "        \n",
    "        # Add skip connection\n",
    "        x = Add(name=f'{name_prefix}_add')([x, shortcut])\n",
    "        x = Activation('relu', name=f'{name_prefix}_relu2')(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # Input\n",
    "    inputs = Input(shape=input_shape, name='input')\n",
    "    \n",
    "    # Initial convolution\n",
    "    x = Conv2D(64, 7, strides=2, padding='same', name='conv1')(inputs)\n",
    "    x = BatchNormalization(name='bn1')(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "    x = MaxPooling2D(3, strides=2, padding='same', name='pool1')(x)\n",
    "    \n",
    "    # Residual blocks\n",
    "    x = residual_block(x, 64, name_prefix='res2a')\n",
    "    x = residual_block(x, 64, name_prefix='res2b')\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = residual_block(x, 128, stride=2, name_prefix='res3a')\n",
    "    x = residual_block(x, 128, name_prefix='res3b')\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = residual_block(x, 256, stride=2, name_prefix='res4a')\n",
    "    x = residual_block(x, 256, name_prefix='res4b')\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = residual_block(x, 512, stride=2, name_prefix='res5a')\n",
    "    x = residual_block(x, 512, name_prefix='res5b')\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Classification head\n",
    "    x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n",
    "    x = Dense(256, name='fc1')(x)\n",
    "    x = BatchNormalization(name='bn_fc1')(x)\n",
    "    x = Activation('relu', name='relu_fc1')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs, name='VehicleResNet')\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"âœ… VehicleResNet architecture function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13947cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model - Choose one architecture\n",
    "print(\"ğŸ—ï¸  Building CNN Model...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Choose architecture (uncomment the one you want to use):\n",
    "# Option 1: Full VehicleNet (more parameters, higher accuracy potential)\n",
    "model = build_vehicle_cnn()\n",
    "\n",
    "# Option 2: Lite version (faster training, fewer parameters)\n",
    "# model = build_vehicle_cnn_lite()\n",
    "\n",
    "# Option 3: ResNet-style with skip connections\n",
    "# model = build_vehicle_resnet()\n",
    "\n",
    "print(f\"âœ… Model '{model.name}' built successfully!\")\n",
    "print(f\"   Input shape: {Config.IMG_SHAPE}\")\n",
    "print(f\"   Output classes: {Config.NUM_CLASSES} ({Config.CLASSES})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11c8ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Model Summary\n",
    "print(\"ğŸ“Š Model Architecture Summary\")\n",
    "print(\"=\"*60)\n",
    "model.summary()\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.size(w).numpy() for w in model.trainable_weights])\n",
    "non_trainable_params = total_params - trainable_params\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ğŸ“ˆ Parameter Count:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Non-trainable parameters: {non_trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a180777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Model Architecture\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def visualize_model_architecture(model, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a visual diagram of the model architecture.\n",
    "    \"\"\"\n",
    "    if save_path is None:\n",
    "        save_path = Config.RESULTS_DIR / f'{model.name}_architecture.png'\n",
    "    \n",
    "    try:\n",
    "        plot_model(\n",
    "            model,\n",
    "            to_file=str(save_path),\n",
    "            show_shapes=True,\n",
    "            show_layer_names=True,\n",
    "            rankdir='TB',  # Top to bottom\n",
    "            expand_nested=True,\n",
    "            dpi=150\n",
    "        )\n",
    "        print(f\"âœ… Model architecture diagram saved to: {save_path}\")\n",
    "        \n",
    "        # Display the image\n",
    "        from IPython.display import Image, display\n",
    "        display(Image(filename=str(save_path)))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  Could not create architecture diagram: {e}\")\n",
    "        print(\"   Install graphviz: sudo apt-get install graphviz\")\n",
    "        print(\"   Or: pip install pydot graphviz\")\n",
    "\n",
    "# Visualize the architecture\n",
    "visualize_model_architecture(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471fc520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "def compile_model(model, learning_rate=Config.LEARNING_RATE):\n",
    "    \"\"\"\n",
    "    Compile the model with optimizer, loss function, and metrics.\n",
    "    \"\"\"\n",
    "    # Optimizer - Adam with learning rate\n",
    "    optimizer = Adam(\n",
    "        learning_rate=learning_rate,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07\n",
    "    )\n",
    "    \n",
    "    # Alternative optimizers (uncomment to use):\n",
    "    # optimizer = SGD(learning_rate=learning_rate, momentum=0.9, nesterov=True)\n",
    "    # optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    \n",
    "    # Loss function\n",
    "    loss = CategoricalCrossentropy(label_smoothing=0.1)  # Label smoothing for regularization\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = [\n",
    "        'accuracy',\n",
    "        tf.keras.metrics.Precision(name='precision'),\n",
    "        tf.keras.metrics.Recall(name='recall'),\n",
    "        tf.keras.metrics.AUC(name='auc', multi_label=True)\n",
    "    ]\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile the model\n",
    "model = compile_model(model)\n",
    "\n",
    "print(\"âœ… Model compiled!\")\n",
    "print(f\"\\nğŸ“‹ Compilation Settings:\")\n",
    "print(f\"   Optimizer: Adam (lr={Config.LEARNING_RATE})\")\n",
    "print(f\"   Loss: Categorical Crossentropy (label smoothing=0.1)\")\n",
    "print(f\"   Metrics: accuracy, precision, recall, AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbb10ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Training Callbacks\n",
    "from tensorflow.keras.callbacks import (\n",
    "    ModelCheckpoint, EarlyStopping, ReduceLROnPlateau,\n",
    "    TensorBoard, CSVLogger, LearningRateScheduler\n",
    ")\n",
    "import datetime\n",
    "\n",
    "def create_callbacks():\n",
    "    \"\"\"\n",
    "    Create callbacks for training monitoring and optimization.\n",
    "    \"\"\"\n",
    "    callbacks_list = []\n",
    "    \n",
    "    # 1. Model Checkpoint - Save best model\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=str(Config.MODELS_DIR / 'vehicle_cnn_best.keras'),\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=False,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks_list.append(checkpoint)\n",
    "    \n",
    "    # 2. Early Stopping - Prevent overfitting\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        patience=Config.EARLY_STOPPING_PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks_list.append(early_stopping)\n",
    "    \n",
    "    # 3. Reduce Learning Rate on Plateau\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        factor=0.5,           # Reduce LR by half\n",
    "        patience=5,           # Wait 5 epochs before reducing\n",
    "        min_lr=1e-7,          # Minimum learning rate\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks_list.append(reduce_lr)\n",
    "    \n",
    "    # 4. TensorBoard for visualization\n",
    "    log_dir = Config.RESULTS_DIR / 'tensorboard' / datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard = TensorBoard(\n",
    "        log_dir=str(log_dir),\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True,\n",
    "        update_freq='epoch'\n",
    "    )\n",
    "    callbacks_list.append(tensorboard)\n",
    "    \n",
    "    # 5. CSV Logger - Log training history\n",
    "    csv_logger = CSVLogger(\n",
    "        filename=str(Config.RESULTS_DIR / 'training_history.csv'),\n",
    "        separator=',',\n",
    "        append=False\n",
    "    )\n",
    "    callbacks_list.append(csv_logger)\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = create_callbacks()\n",
    "\n",
    "print(\"âœ… Training callbacks created:\")\n",
    "print(\"   ğŸ“ ModelCheckpoint - Saves best model\")\n",
    "print(\"   ğŸ›‘ EarlyStopping - Stops if no improvement\")\n",
    "print(\"   ğŸ“‰ ReduceLROnPlateau - Reduces learning rate\")\n",
    "print(\"   ğŸ“Š TensorBoard - Training visualization\")\n",
    "print(\"   ğŸ“ CSVLogger - Logs training history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178fe8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Model with a Test Forward Pass\n",
    "print(\"ğŸ” Testing model with a sample batch...\")\n",
    "\n",
    "# Get a sample batch from training data\n",
    "for sample_images, sample_labels in train_dataset.take(1):\n",
    "    print(f\"   Input shape: {sample_images.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    predictions = model.predict(sample_images, verbose=0)\n",
    "    \n",
    "    print(f\"   Output shape: {predictions.shape}\")\n",
    "    print(f\"   Output range: [{predictions.min():.4f}, {predictions.max():.4f}]\")\n",
    "    print(f\"   Sum of probabilities (should be ~1): {predictions[0].sum():.4f}\")\n",
    "    \n",
    "    # Show sample prediction\n",
    "    pred_class = np.argmax(predictions[0])\n",
    "    pred_prob = predictions[0][pred_class]\n",
    "    true_class = np.argmax(sample_labels[0])\n",
    "    \n",
    "    print(f\"\\n   Sample prediction:\")\n",
    "    print(f\"   True class: {Config.CLASSES[true_class]}\")\n",
    "    print(f\"   Predicted: {Config.CLASSES[pred_class]} ({pred_prob*100:.1f}%)\")\n",
    "    \n",
    "print(\"\\nâœ… Model forward pass successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef060cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of Model Architecture\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“‹ STEP 4 SUMMARY: CNN Model Architecture\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ—ï¸  Model: {model.name}\n",
    "    \n",
    "ğŸ“ Architecture:\n",
    "    â”œâ”€â”€ Input: {Config.IMG_SHAPE}\n",
    "    â”œâ”€â”€ Conv Block 1: 32 filters (3Ã—3) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout\n",
    "    â”œâ”€â”€ Conv Block 2: 64 filters (3Ã—3) â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout\n",
    "    â”œâ”€â”€ Conv Block 3: 128 filters (3Ã—3) Ã— 3 â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout\n",
    "    â”œâ”€â”€ Conv Block 4: 256 filters (3Ã—3) Ã— 3 â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout\n",
    "    â”œâ”€â”€ Conv Block 5: 512 filters (3Ã—3) Ã— 2 â†’ BatchNorm â†’ ReLU â†’ MaxPool â†’ Dropout\n",
    "    â”œâ”€â”€ Global Average Pooling\n",
    "    â”œâ”€â”€ Dense: 512 â†’ BatchNorm â†’ ReLU â†’ Dropout(0.5)\n",
    "    â”œâ”€â”€ Dense: 256 â†’ BatchNorm â†’ ReLU â†’ Dropout(0.5)\n",
    "    â””â”€â”€ Output: {Config.NUM_CLASSES} classes (softmax)\n",
    "\n",
    "ğŸ“Š Parameters: {model.count_params():,}\n",
    "\n",
    "âš™ï¸  Compilation:\n",
    "    â”œâ”€â”€ Optimizer: Adam (lr={Config.LEARNING_RATE})\n",
    "    â”œâ”€â”€ Loss: Categorical Crossentropy (label smoothing=0.1)\n",
    "    â””â”€â”€ Metrics: accuracy, precision, recall, AUC\n",
    "\n",
    "ğŸ”§ Callbacks:\n",
    "    â”œâ”€â”€ ModelCheckpoint (save best model)\n",
    "    â”œâ”€â”€ EarlyStopping (patience={Config.EARLY_STOPPING_PATIENCE})\n",
    "    â”œâ”€â”€ ReduceLROnPlateau (factor=0.5, patience=5)\n",
    "    â”œâ”€â”€ TensorBoard (visualization)\n",
    "    â””â”€â”€ CSVLogger (training history)\n",
    "\n",
    "âœ… Model is ready for training!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f7914",
   "metadata": {},
   "source": [
    "### Step 4 Complete! âœ…\n",
    "\n",
    "We have successfully built a custom CNN architecture from scratch:\n",
    "\n",
    "**Three Architecture Options:**\n",
    "1. **VehicleNet** - Full architecture with 5 conv blocks (~8M parameters)\n",
    "2. **VehicleNet_Lite** - Lighter version for faster training (~1M parameters)\n",
    "3. **VehicleResNet** - ResNet-style with skip connections (~4M parameters)\n",
    "\n",
    "**Architecture Highlights:**\n",
    "- Progressive filter depth: 32 â†’ 64 â†’ 128 â†’ 256 â†’ 512\n",
    "- Batch Normalization after every convolution\n",
    "- Dropout layers for regularization (0.25 â†’ 0.5)\n",
    "- Global Average Pooling instead of Flatten\n",
    "- Label smoothing (0.1) for better generalization\n",
    "\n",
    "**Training Setup:**\n",
    "- Optimizer: Adam with learning rate scheduling\n",
    "- Early stopping to prevent overfitting\n",
    "- Best model checkpointing\n",
    "- TensorBoard logging for visualization\n",
    "\n",
    "**Next Step:** Step 5 - Model Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3eb6c8",
   "metadata": {},
   "source": [
    "## Step 5: Model Training\n",
    "\n",
    "Now we'll train our CNN model on the vehicle dataset. This step includes:\n",
    "\n",
    "1. **Training Loop** - Train the model with augmented data\n",
    "2. **Progress Monitoring** - Track accuracy and loss during training\n",
    "3. **Learning Rate Scheduling** - Automatic LR adjustment\n",
    "4. **Model Checkpointing** - Save the best model\n",
    "5. **Training History Visualization** - Plot training curves\n",
    "\n",
    "### Training Tips:\n",
    "- Start with fewer epochs to verify everything works\n",
    "- Monitor validation loss for overfitting\n",
    "- Use early stopping to save training time\n",
    "- Check GPU utilization for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f301706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training Checklist\n",
    "print(\"ğŸ” Pre-Training Checklist\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check data\n",
    "print(f\"\\nğŸ“Š Data Ready:\")\n",
    "print(f\"   âœ… Training samples: {len(X_train)}\")\n",
    "print(f\"   âœ… Validation samples: {len(X_val)}\")\n",
    "print(f\"   âœ… Test samples: {len(X_test)}\")\n",
    "\n",
    "# Check model\n",
    "print(f\"\\nğŸ—ï¸  Model Ready:\")\n",
    "print(f\"   âœ… Model name: {model.name}\")\n",
    "print(f\"   âœ… Parameters: {model.count_params():,}\")\n",
    "print(f\"   âœ… Compiled: {model.optimizer is not None}\")\n",
    "\n",
    "# Check callbacks\n",
    "print(f\"\\nğŸ”§ Callbacks Ready:\")\n",
    "print(f\"   âœ… Number of callbacks: {len(callbacks)}\")\n",
    "\n",
    "# Training configuration\n",
    "print(f\"\\nâš™ï¸  Training Configuration:\")\n",
    "print(f\"   â€¢ Epochs: {Config.EPOCHS}\")\n",
    "print(f\"   â€¢ Batch Size: {Config.BATCH_SIZE}\")\n",
    "print(f\"   â€¢ Learning Rate: {Config.LEARNING_RATE}\")\n",
    "print(f\"   â€¢ Early Stopping Patience: {Config.EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "# GPU check\n",
    "print(f\"\\nğŸ–¥ï¸  Hardware:\")\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"   âœ… GPU available and will be used for training\")\n",
    "else:\n",
    "    print(f\"   âš ï¸  No GPU detected - training will be slower on CPU\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… All checks passed! Ready to start training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95ebf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "import time\n",
    "\n",
    "print(\"ğŸš€ Starting Model Training...\")\n",
    "print(\"=\"*60)\n",
    "print(f\"   Model: {model.name}\")\n",
    "print(f\"   Epochs: {Config.EPOCHS}\")\n",
    "print(f\"   Batch Size: {Config.BATCH_SIZE}\")\n",
    "print(f\"   Training Samples: {len(X_train)}\")\n",
    "print(f\"   Validation Samples: {len(X_val)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=Config.EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Calculate training time\n",
    "training_time = time.time() - start_time\n",
    "hours = int(training_time // 3600)\n",
    "minutes = int((training_time % 3600) // 60)\n",
    "seconds = int(training_time % 60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… Training Complete!\")\n",
    "print(f\"   Total training time: {hours}h {minutes}m {seconds}s\")\n",
    "print(f\"   Final training accuracy: {history.history['accuracy'][-1]*100:.2f}%\")\n",
    "print(f\"   Final validation accuracy: {history.history['val_accuracy'][-1]*100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d464eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training History - Loss and Accuracy\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss/accuracy curves.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    epochs_range = range(1, len(history.history['loss']) + 1)\n",
    "    \n",
    "    # Plot 1: Training & Validation Loss\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(epochs_range, history.history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs_range, history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark best epoch\n",
    "    best_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    ax1.axvline(x=best_epoch, color='green', linestyle='--', alpha=0.7, label=f'Best Epoch: {best_epoch}')\n",
    "    ax1.scatter([best_epoch], [best_val_loss], color='green', s=100, zorder=5)\n",
    "    \n",
    "    # Plot 2: Training & Validation Accuracy\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(epochs_range, history.history['accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs_range, history.history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend(loc='lower right')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Mark best accuracy\n",
    "    best_acc_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    ax2.axvline(x=best_acc_epoch, color='green', linestyle='--', alpha=0.7)\n",
    "    ax2.scatter([best_acc_epoch], [best_val_acc], color='green', s=100, zorder=5)\n",
    "    ax2.annotate(f'{best_val_acc*100:.1f}%', (best_acc_epoch, best_val_acc), \n",
    "                 textcoords=\"offset points\", xytext=(10, 5), fontsize=10)\n",
    "    \n",
    "    # Plot 3: Precision and Recall\n",
    "    ax3 = axes[1, 0]\n",
    "    if 'precision' in history.history:\n",
    "        ax3.plot(epochs_range, history.history['precision'], 'b-', label='Training Precision', linewidth=2)\n",
    "        ax3.plot(epochs_range, history.history['val_precision'], 'b--', label='Val Precision', linewidth=2)\n",
    "    if 'recall' in history.history:\n",
    "        ax3.plot(epochs_range, history.history['recall'], 'r-', label='Training Recall', linewidth=2)\n",
    "        ax3.plot(epochs_range, history.history['val_recall'], 'r--', label='Val Recall', linewidth=2)\n",
    "    ax3.set_title('Precision and Recall', fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Score')\n",
    "    ax3.legend(loc='lower right')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Learning Rate (if available) or AUC\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'auc' in history.history:\n",
    "        ax4.plot(epochs_range, history.history['auc'], 'g-', label='Training AUC', linewidth=2)\n",
    "        ax4.plot(epochs_range, history.history['val_auc'], 'g--', label='Validation AUC', linewidth=2)\n",
    "        ax4.set_title('AUC Score', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('AUC')\n",
    "        ax4.legend(loc='lower right')\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Show learning rate changes if available\n",
    "        ax4.text(0.5, 0.5, 'AUC data not available', ha='center', va='center', \n",
    "                transform=ax4.transAxes, fontsize=12)\n",
    "        ax4.axis('off')\n",
    "    \n",
    "    plt.suptitle(f'{model.name} Training History', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"ğŸ’¾ Training history plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(history, save_path=Config.RESULTS_DIR / 'training_history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Training Progress - Detect Overfitting\n",
    "def analyze_training(history):\n",
    "    \"\"\"\n",
    "    Analyze training progress and detect potential issues.\n",
    "    \"\"\"\n",
    "    train_acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    print(\"ğŸ“Š Training Analysis\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Best epoch\n",
    "    best_epoch = np.argmin(val_loss) + 1\n",
    "    print(f\"\\nğŸ† Best Epoch: {best_epoch}\")\n",
    "    print(f\"   Best Validation Loss: {val_loss[best_epoch-1]:.4f}\")\n",
    "    print(f\"   Best Validation Accuracy: {val_acc[best_epoch-1]*100:.2f}%\")\n",
    "    \n",
    "    # Final metrics\n",
    "    print(f\"\\nğŸ“ˆ Final Metrics (Epoch {len(train_acc)}):\")\n",
    "    print(f\"   Training Loss: {train_loss[-1]:.4f}\")\n",
    "    print(f\"   Validation Loss: {val_loss[-1]:.4f}\")\n",
    "    print(f\"   Training Accuracy: {train_acc[-1]*100:.2f}%\")\n",
    "    print(f\"   Validation Accuracy: {val_acc[-1]*100:.2f}%\")\n",
    "    \n",
    "    # Overfitting analysis\n",
    "    print(f\"\\nğŸ” Overfitting Analysis:\")\n",
    "    acc_gap = train_acc[-1] - val_acc[-1]\n",
    "    loss_gap = val_loss[-1] - train_loss[-1]\n",
    "    \n",
    "    if acc_gap > 0.15:\n",
    "        print(f\"   âš ï¸  Large accuracy gap ({acc_gap*100:.1f}%) - Model may be overfitting\")\n",
    "    elif acc_gap > 0.05:\n",
    "        print(f\"   âš¡ Moderate accuracy gap ({acc_gap*100:.1f}%) - Some overfitting present\")\n",
    "    else:\n",
    "        print(f\"   âœ… Accuracy gap ({acc_gap*100:.1f}%) - Good generalization\")\n",
    "    \n",
    "    if loss_gap > 0.5:\n",
    "        print(f\"   âš ï¸  Large loss gap ({loss_gap:.4f}) - Consider more regularization\")\n",
    "    else:\n",
    "        print(f\"   âœ… Loss gap ({loss_gap:.4f}) - Acceptable\")\n",
    "    \n",
    "    # Convergence check\n",
    "    print(f\"\\nğŸ“‰ Convergence Check:\")\n",
    "    last_5_val_loss = val_loss[-5:] if len(val_loss) >= 5 else val_loss\n",
    "    loss_std = np.std(last_5_val_loss)\n",
    "    \n",
    "    if loss_std < 0.01:\n",
    "        print(f\"   âœ… Model has converged (loss std: {loss_std:.4f})\")\n",
    "    elif loss_std < 0.05:\n",
    "        print(f\"   âš¡ Model is stabilizing (loss std: {loss_std:.4f})\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸  Model may benefit from more training (loss std: {loss_std:.4f})\")\n",
    "    \n",
    "    # Improvement rate\n",
    "    if len(val_acc) > 10:\n",
    "        early_acc = np.mean(val_acc[:5])\n",
    "        late_acc = np.mean(val_acc[-5:])\n",
    "        improvement = late_acc - early_acc\n",
    "        print(f\"\\nğŸ“Š Improvement: {improvement*100:.1f}% from early to late epochs\")\n",
    "\n",
    "analyze_training(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a85e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Trained Model\n",
    "def save_model(model, history, save_dir=Config.MODELS_DIR):\n",
    "    \"\"\"\n",
    "    Save the trained model and training history.\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Save model in Keras format\n",
    "    model_path = save_dir / f'{model.name}_final.keras'\n",
    "    model.save(model_path)\n",
    "    print(f\"âœ… Model saved: {model_path}\")\n",
    "    \n",
    "    # Save model weights only\n",
    "    weights_path = save_dir / f'{model.name}_weights.weights.h5'\n",
    "    model.save_weights(weights_path)\n",
    "    print(f\"âœ… Weights saved: {weights_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    history_df = pd.DataFrame(history.history)\n",
    "    history_path = save_dir / 'training_history.csv'\n",
    "    history_df.to_csv(history_path, index=False)\n",
    "    print(f\"âœ… Training history saved: {history_path}\")\n",
    "    \n",
    "    # Save model configuration\n",
    "    config_path = save_dir / 'model_config.json'\n",
    "    with open(config_path, 'w') as f:\n",
    "        import json\n",
    "        config = {\n",
    "            'model_name': model.name,\n",
    "            'input_shape': list(Config.IMG_SHAPE),\n",
    "            'num_classes': Config.NUM_CLASSES,\n",
    "            'classes': Config.CLASSES,\n",
    "            'epochs_trained': len(history.history['loss']),\n",
    "            'final_train_accuracy': float(history.history['accuracy'][-1]),\n",
    "            'final_val_accuracy': float(history.history['val_accuracy'][-1]),\n",
    "            'best_val_accuracy': float(max(history.history['val_accuracy'])),\n",
    "            'best_epoch': int(np.argmax(history.history['val_accuracy']) + 1)\n",
    "        }\n",
    "        json.dump(config, f, indent=2)\n",
    "    print(f\"âœ… Model config saved: {config_path}\")\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "# Save the model\n",
    "model_path = save_model(model, history)\n",
    "print(f\"\\nğŸ“ All files saved to: {Config.MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Best Model (saved by ModelCheckpoint)\n",
    "def load_best_model():\n",
    "    \"\"\"\n",
    "    Load the best model saved during training.\n",
    "    \"\"\"\n",
    "    best_model_path = Config.MODELS_DIR / 'vehicle_cnn_best.keras'\n",
    "    \n",
    "    if best_model_path.exists():\n",
    "        print(f\"ğŸ“‚ Loading best model from: {best_model_path}\")\n",
    "        best_model = tf.keras.models.load_model(best_model_path)\n",
    "        print(f\"âœ… Best model loaded successfully!\")\n",
    "        return best_model\n",
    "    else:\n",
    "        print(f\"âš ï¸  Best model not found at: {best_model_path}\")\n",
    "        print(\"   Using the current model instead.\")\n",
    "        return model\n",
    "\n",
    "# Load the best model for evaluation\n",
    "best_model = load_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f418619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Training vs Best Model Performance\n",
    "print(\"ğŸ“Š Model Comparison: Final vs Best\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate final model on validation set\n",
    "print(\"\\nğŸ”„ Evaluating final model...\")\n",
    "final_results = model.evaluate(val_dataset, verbose=0)\n",
    "final_metrics = dict(zip(model.metrics_names, final_results))\n",
    "\n",
    "print(f\"\\nğŸ“ˆ Final Model (Last Epoch):\")\n",
    "for name, value in final_metrics.items():\n",
    "    if 'accuracy' in name or 'precision' in name or 'recall' in name or 'auc' in name:\n",
    "        print(f\"   {name}: {value*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"   {name}: {value:.4f}\")\n",
    "\n",
    "# Evaluate best model on validation set\n",
    "print(\"\\nğŸ”„ Evaluating best model...\")\n",
    "best_results = best_model.evaluate(val_dataset, verbose=0)\n",
    "best_metrics = dict(zip(best_model.metrics_names, best_results))\n",
    "\n",
    "print(f\"\\nğŸ† Best Model (Checkpoint):\")\n",
    "for name, value in best_metrics.items():\n",
    "    if 'accuracy' in name or 'precision' in name or 'recall' in name or 'auc' in name:\n",
    "        print(f\"   {name}: {value*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"   {name}: {value:.4f}\")\n",
    "\n",
    "# Recommendation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "if best_metrics.get('accuracy', 0) >= final_metrics.get('accuracy', 0):\n",
    "    print(\"âœ… Recommendation: Use the BEST model for inference\")\n",
    "    model_to_use = best_model\n",
    "else:\n",
    "    print(\"âœ… Recommendation: Use the FINAL model for inference\")\n",
    "    model_to_use = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd739d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Summary\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“‹ STEP 5 SUMMARY: Model Training\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get training stats\n",
    "epochs_trained = len(history.history['loss'])\n",
    "best_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ‹ï¸  Training Complete!\n",
    "\n",
    "ğŸ“Š Training Statistics:\n",
    "    â”œâ”€â”€ Epochs Trained: {epochs_trained}\n",
    "    â”œâ”€â”€ Best Epoch: {best_epoch}\n",
    "    â”œâ”€â”€ Best Validation Accuracy: {best_val_acc*100:.2f}%\n",
    "    â”œâ”€â”€ Final Validation Accuracy: {final_val_acc*100:.2f}%\n",
    "    â””â”€â”€ Training Time: {hours}h {minutes}m {seconds}s\n",
    "\n",
    "ğŸ’¾ Saved Files:\n",
    "    â”œâ”€â”€ Best Model: {Config.MODELS_DIR}/vehicle_cnn_best.keras\n",
    "    â”œâ”€â”€ Final Model: {Config.MODELS_DIR}/{model.name}_final.keras\n",
    "    â”œâ”€â”€ Weights: {Config.MODELS_DIR}/{model.name}_weights.weights.h5\n",
    "    â”œâ”€â”€ Training History: {Config.MODELS_DIR}/training_history.csv\n",
    "    â””â”€â”€ Config: {Config.MODELS_DIR}/model_config.json\n",
    "\n",
    "ğŸ“ˆ Training Curves:\n",
    "    â””â”€â”€ Saved to: {Config.RESULTS_DIR}/training_history.png\n",
    "\n",
    "âœ… Model is trained and ready for evaluation!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae47fdb0",
   "metadata": {},
   "source": [
    "### Step 5 Complete! âœ…\n",
    "\n",
    "We have successfully trained the CNN model:\n",
    "\n",
    "**Training Completed:**\n",
    "1. âœ… Pre-training checklist verified\n",
    "2. âœ… Model trained with augmented data\n",
    "3. âœ… Training history plotted (loss, accuracy, precision, recall, AUC)\n",
    "4. âœ… Overfitting analysis performed\n",
    "5. âœ… Model saved in multiple formats\n",
    "6. âœ… Best model loaded for evaluation\n",
    "\n",
    "**Saved Artifacts:**\n",
    "- `vehicle_cnn_best.keras` - Best model checkpoint\n",
    "- `VehicleNet_final.keras` - Final trained model\n",
    "- `training_history.csv` - Complete training logs\n",
    "- `training_history.png` - Training curves visualization\n",
    "\n",
    "**Key Metrics:**\n",
    "- Training accuracy, validation accuracy\n",
    "- Precision, Recall, AUC\n",
    "- Loss convergence analysis\n",
    "\n",
    "**Next Step:** Step 6 - Model Evaluation and Metrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55302492",
   "metadata": {},
   "source": [
    "## Step 6: Model Evaluation and Metrics\n",
    "\n",
    "Now we'll comprehensively evaluate our trained model on the test set. This includes:\n",
    "\n",
    "1. **Test Set Evaluation** - Final performance metrics\n",
    "2. **Confusion Matrix** - Visualize classification errors\n",
    "3. **Classification Report** - Per-class precision, recall, F1-score\n",
    "4. **ROC Curves** - Receiver Operating Characteristic for each class\n",
    "5. **Precision-Recall Curves** - PR analysis\n",
    "6. **Per-Class Analysis** - Detailed breakdown by vehicle type\n",
    "\n",
    "### Why These Metrics Matter:\n",
    "- **Accuracy**: Overall correctness\n",
    "- **Precision**: How many predicted positives are correct\n",
    "- **Recall**: How many actual positives were found\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **AUC-ROC**: Model's ability to distinguish between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a8175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on Test Set\n",
    "print(\"ğŸ§ª Evaluating Model on Test Set\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the best model for evaluation\n",
    "eval_model = best_model if 'best_model' in dir() else model\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = eval_model.evaluate(test_dataset, verbose=1)\n",
    "test_metrics = dict(zip(eval_model.metrics_names, test_results))\n",
    "\n",
    "print(\"\\nğŸ“Š Test Set Results:\")\n",
    "print(\"-\"*40)\n",
    "for name, value in test_metrics.items():\n",
    "    if 'accuracy' in name or 'precision' in name or 'recall' in name or 'auc' in name:\n",
    "        print(f\"   {name.capitalize():15}: {value*100:.2f}%\")\n",
    "    else:\n",
    "        print(f\"   {name.capitalize():15}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61ad3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Predictions for Detailed Analysis\n",
    "print(\"ğŸ”® Generating predictions on test set...\")\n",
    "\n",
    "# Get predictions\n",
    "y_pred_proba = eval_model.predict(X_test, verbose=1)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "y_true = y_test  # Original labels (not one-hot)\n",
    "\n",
    "print(f\"\\nâœ… Predictions generated!\")\n",
    "print(f\"   Test samples: {len(y_true)}\")\n",
    "print(f\"   Prediction shape: {y_pred_proba.shape}\")\n",
    "print(f\"   Classes: {Config.CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix with optional normalization.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        fmt = '.2f'\n",
    "        title = 'Normalized Confusion Matrix'\n",
    "    else:\n",
    "        fmt = 'd'\n",
    "        title = 'Confusion Matrix'\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=[c.capitalize() for c in classes],\n",
    "           yticklabels=[c.capitalize() for c in classes],\n",
    "           title=title,\n",
    "           ylabel='True Label',\n",
    "           xlabel='Predicted Label')\n",
    "    \n",
    "    # Rotate x labels\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                   ha=\"center\", va=\"center\",\n",
    "                   color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                   fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"ğŸ’¾ Confusion matrix saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "# Plot both normalized and non-normalized confusion matrices\n",
    "print(\"ğŸ“Š Confusion Matrix (Counts)\")\n",
    "cm = plot_confusion_matrix(y_true, y_pred, Config.CLASSES, normalize=False,\n",
    "                           save_path=Config.RESULTS_DIR / 'confusion_matrix.png')\n",
    "\n",
    "print(\"\\nğŸ“Š Confusion Matrix (Normalized)\")\n",
    "cm_norm = plot_confusion_matrix(y_true, y_pred, Config.CLASSES, normalize=True,\n",
    "                                save_path=Config.RESULTS_DIR / 'confusion_matrix_normalized.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314aa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"ğŸ“‹ Classification Report\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=[c.capitalize() for c in Config.CLASSES],\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "# Also get as dictionary for further analysis\n",
    "report_dict = classification_report(\n",
    "    y_true, y_pred,\n",
    "    target_names=[c.capitalize() for c in Config.CLASSES],\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "# Save report\n",
    "report_df = pd.DataFrame(report_dict).transpose()\n",
    "report_df.to_csv(Config.RESULTS_DIR / 'classification_report.csv')\n",
    "print(f\"\\nğŸ’¾ Classification report saved to: {Config.RESULTS_DIR / 'classification_report.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee35751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Class Metrics Visualization\n",
    "def plot_per_class_metrics(report_dict, classes, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize precision, recall, and F1-score for each class.\n",
    "    \"\"\"\n",
    "    # Extract metrics for each class\n",
    "    precision = [report_dict[c.capitalize()]['precision'] for c in classes]\n",
    "    recall = [report_dict[c.capitalize()]['recall'] for c in classes]\n",
    "    f1_score = [report_dict[c.capitalize()]['f1-score'] for c in classes]\n",
    "    support = [report_dict[c.capitalize()]['support'] for c in classes]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot 1: Bar chart of metrics\n",
    "    ax1 = axes[0]\n",
    "    x = np.arange(len(classes))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = ax1.bar(x - width, precision, width, label='Precision', color='steelblue')\n",
    "    bars2 = ax1.bar(x, recall, width, label='Recall', color='orange')\n",
    "    bars3 = ax1.bar(x + width, f1_score, width, label='F1-Score', color='green')\n",
    "    \n",
    "    ax1.set_xlabel('Vehicle Class', fontsize=12)\n",
    "    ax1.set_ylabel('Score', fontsize=12)\n",
    "    ax1.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([c.capitalize() for c in classes])\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 1.1)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bars in [bars1, bars2, bars3]:\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            ax1.annotate(f'{height:.2f}',\n",
    "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                        xytext=(0, 3),\n",
    "                        textcoords=\"offset points\",\n",
    "                        ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Plot 2: Radar chart\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    # Prepare data for radar chart\n",
    "    categories = ['Precision', 'Recall', 'F1-Score']\n",
    "    \n",
    "    # Create radar chart\n",
    "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(classes)))\n",
    "    \n",
    "    ax2 = plt.subplot(122, polar=True)\n",
    "    \n",
    "    for idx, vehicle_class in enumerate(classes):\n",
    "        values = [precision[idx], recall[idx], f1_score[idx]]\n",
    "        values += values[:1]  # Complete the circle\n",
    "        ax2.plot(angles, values, 'o-', linewidth=2, label=vehicle_class.capitalize(), color=colors[idx])\n",
    "        ax2.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "    \n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title('Performance Radar Chart', fontsize=14, fontweight='bold', y=1.1)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"ğŸ’¾ Per-class metrics plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot per-class metrics\n",
    "plot_per_class_metrics(report_dict, Config.CLASSES, \n",
    "                       save_path=Config.RESULTS_DIR / 'per_class_metrics.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff832e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for Each Class\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_roc_curves(y_true, y_pred_proba, classes, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for each class (One-vs-Rest).\n",
    "    \"\"\"\n",
    "    # Binarize labels for multi-class ROC\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(classes)))\n",
    "    \n",
    "    # Compute ROC curve and AUC for each class\n",
    "    fpr = {}\n",
    "    tpr = {}\n",
    "    roc_auc = {}\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "    # Compute micro-average ROC curve\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true_bin.ravel(), y_pred_proba.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Plot ROC for each class\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(classes)))\n",
    "    \n",
    "    for i, (class_name, color) in enumerate(zip(classes, colors)):\n",
    "        ax.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "                label=f'{class_name.capitalize()} (AUC = {roc_auc[i]:.3f})')\n",
    "    \n",
    "    # Plot micro-average\n",
    "    ax.plot(fpr[\"micro\"], tpr[\"micro\"], color='navy', linestyle='--', lw=3,\n",
    "            label=f'Micro-average (AUC = {roc_auc[\"micro\"]:.3f})')\n",
    "    \n",
    "    # Plot diagonal\n",
    "    ax.plot([0, 1], [0, 1], 'k--', lw=1, label='Random Classifier')\n",
    "    \n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title('ROC Curves (One-vs-Rest)', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"ğŸ’¾ ROC curves saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc\n",
    "\n",
    "# Plot ROC curves\n",
    "roc_auc_scores = plot_roc_curves(y_true, y_pred_proba, Config.CLASSES,\n",
    "                                 save_path=Config.RESULTS_DIR / 'roc_curves.png')\n",
    "\n",
    "print(\"\\nğŸ“Š AUC Scores per Class:\")\n",
    "for i, class_name in enumerate(Config.CLASSES):\n",
    "    print(f\"   {class_name.capitalize():12}: {roc_auc_scores[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curves\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_precision_recall_curves(y_true, y_pred_proba, classes, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for each class.\n",
    "    \"\"\"\n",
    "    # Binarize labels\n",
    "    y_true_bin = label_binarize(y_true, classes=range(len(classes)))\n",
    "    \n",
    "    # Compute PR curve and AP for each class\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    avg_precision = {}\n",
    "    \n",
    "    for i, class_name in enumerate(classes):\n",
    "        precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "        avg_precision[i] = average_precision_score(y_true_bin[:, i], y_pred_proba[:, i])\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(classes)))\n",
    "    \n",
    "    for i, (class_name, color) in enumerate(zip(classes, colors)):\n",
    "        ax.plot(recall[i], precision[i], color=color, lw=2,\n",
    "                label=f'{class_name.capitalize()} (AP = {avg_precision[i]:.3f})')\n",
    "    \n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('Recall', fontsize=12)\n",
    "    ax.set_ylabel('Precision', fontsize=12)\n",
    "    ax.set_title('Precision-Recall Curves', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"ğŸ’¾ PR curves saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return avg_precision\n",
    "\n",
    "# Plot PR curves\n",
    "avg_precision_scores = plot_precision_recall_curves(y_true, y_pred_proba, Config.CLASSES,\n",
    "                                                    save_path=Config.RESULTS_DIR / 'pr_curves.png')\n",
    "\n",
    "print(\"\\nğŸ“Š Average Precision per Class:\")\n",
    "for i, class_name in enumerate(Config.CLASSES):\n",
    "    print(f\"   {class_name.capitalize():12}: {avg_precision_scores[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d7886c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Prediction Confidence\n",
    "def analyze_prediction_confidence(y_pred_proba, y_true, y_pred, classes):\n",
    "    \"\"\"\n",
    "    Analyze the confidence of predictions.\n",
    "    \"\"\"\n",
    "    # Get max probability (confidence) for each prediction\n",
    "    max_proba = np.max(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Separate correct and incorrect predictions\n",
    "    correct_mask = y_pred == y_true\n",
    "    correct_conf = max_proba[correct_mask]\n",
    "    incorrect_conf = max_proba[~correct_mask]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    # Plot 1: Confidence distribution\n",
    "    ax1 = axes[0]\n",
    "    ax1.hist(correct_conf, bins=30, alpha=0.7, label='Correct', color='green', edgecolor='black')\n",
    "    ax1.hist(incorrect_conf, bins=30, alpha=0.7, label='Incorrect', color='red', edgecolor='black')\n",
    "    ax1.set_xlabel('Prediction Confidence', fontsize=12)\n",
    "    ax1.set_ylabel('Count', fontsize=12)\n",
    "    ax1.set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Confidence by class\n",
    "    ax2 = axes[1]\n",
    "    conf_by_class = [max_proba[y_pred == i] for i in range(len(classes))]\n",
    "    bp = ax2.boxplot(conf_by_class, labels=[c.capitalize() for c in classes], patch_artist=True)\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(classes)))\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "    ax2.set_xlabel('Vehicle Class', fontsize=12)\n",
    "    ax2.set_ylabel('Confidence', fontsize=12)\n",
    "    ax2.set_title('Confidence by Predicted Class', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Accuracy at different confidence thresholds\n",
    "    ax3 = axes[2]\n",
    "    thresholds = np.arange(0.5, 1.0, 0.05)\n",
    "    accuracies = []\n",
    "    coverages = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        mask = max_proba >= thresh\n",
    "        if np.sum(mask) > 0:\n",
    "            acc = np.mean(y_pred[mask] == y_true[mask])\n",
    "            cov = np.mean(mask)\n",
    "        else:\n",
    "            acc = 0\n",
    "            cov = 0\n",
    "        accuracies.append(acc)\n",
    "        coverages.append(cov)\n",
    "    \n",
    "    ax3.plot(thresholds, accuracies, 'b-o', label='Accuracy', linewidth=2)\n",
    "    ax3.plot(thresholds, coverages, 'r-s', label='Coverage', linewidth=2)\n",
    "    ax3.set_xlabel('Confidence Threshold', fontsize=12)\n",
    "    ax3.set_ylabel('Score', fontsize=12)\n",
    "    ax3.set_title('Accuracy vs Coverage at Thresholds', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(Config.RESULTS_DIR / 'confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\nğŸ“Š Confidence Statistics:\")\n",
    "    print(f\"   Overall mean confidence: {np.mean(max_proba):.4f}\")\n",
    "    print(f\"   Correct predictions mean confidence: {np.mean(correct_conf):.4f}\")\n",
    "    if len(incorrect_conf) > 0:\n",
    "        print(f\"   Incorrect predictions mean confidence: {np.mean(incorrect_conf):.4f}\")\n",
    "    print(f\"\\n   High confidence (>0.9) predictions: {np.sum(max_proba > 0.9)} ({np.mean(max_proba > 0.9)*100:.1f}%)\")\n",
    "    print(f\"   Low confidence (<0.6) predictions: {np.sum(max_proba < 0.6)} ({np.mean(max_proba < 0.6)*100:.1f}%)\")\n",
    "\n",
    "# Analyze confidence\n",
    "analyze_prediction_confidence(y_pred_proba, y_true, y_pred, Config.CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07af4c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Correct and Incorrect Predictions\n",
    "def show_predictions(X, y_true, y_pred, y_proba, classes, num_samples=4, show_correct=True):\n",
    "    \"\"\"\n",
    "    Display sample predictions with confidence scores.\n",
    "    \"\"\"\n",
    "    # Filter correct or incorrect predictions\n",
    "    if show_correct:\n",
    "        mask = y_pred == y_true\n",
    "        title = \"âœ… Correct Predictions\"\n",
    "    else:\n",
    "        mask = y_pred != y_true\n",
    "        title = \"âŒ Incorrect Predictions\"\n",
    "    \n",
    "    indices = np.where(mask)[0]\n",
    "    \n",
    "    if len(indices) == 0:\n",
    "        print(f\"No {'correct' if show_correct else 'incorrect'} predictions found.\")\n",
    "        return\n",
    "    \n",
    "    # Select random samples\n",
    "    sample_indices = np.random.choice(indices, min(num_samples, len(indices)), replace=False)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, len(sample_indices), figsize=(4*len(sample_indices), 5))\n",
    "    if len(sample_indices) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (ax, sample_idx) in enumerate(zip(axes, sample_indices)):\n",
    "        img = X[sample_idx]\n",
    "        true_label = classes[y_true[sample_idx]]\n",
    "        pred_label = classes[y_pred[sample_idx]]\n",
    "        confidence = y_proba[sample_idx][y_pred[sample_idx]]\n",
    "        \n",
    "        # Show image\n",
    "        ax.imshow(np.clip(img, 0, 1))\n",
    "        \n",
    "        # Title with prediction info\n",
    "        if show_correct:\n",
    "            ax.set_title(f\"True: {true_label.capitalize()}\\n\"\n",
    "                        f\"Pred: {pred_label.capitalize()}\\n\"\n",
    "                        f\"Conf: {confidence*100:.1f}%\",\n",
    "                        fontsize=11, color='green')\n",
    "        else:\n",
    "            ax.set_title(f\"True: {true_label.capitalize()}\\n\"\n",
    "                        f\"Pred: {pred_label.capitalize()}\\n\"\n",
    "                        f\"Conf: {confidence*100:.1f}%\",\n",
    "                        fontsize=11, color='red')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show correct predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ–¼ï¸  Sample Predictions Visualization\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "show_predictions(X_test, y_true, y_pred, y_pred_proba, Config.CLASSES, \n",
    "                num_samples=4, show_correct=True)\n",
    "\n",
    "show_predictions(X_test, y_true, y_pred, y_pred_proba, Config.CLASSES, \n",
    "                num_samples=4, show_correct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e4a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Evaluation Summary\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ“‹ STEP 6 SUMMARY: Model Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate overall metrics\n",
    "overall_accuracy = np.mean(y_pred == y_true)\n",
    "num_correct = np.sum(y_pred == y_true)\n",
    "num_total = len(y_true)\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ¯ Test Set Performance:\n",
    "\n",
    "ğŸ“Š Overall Metrics:\n",
    "    â”œâ”€â”€ Accuracy: {overall_accuracy*100:.2f}%\n",
    "    â”œâ”€â”€ Correct Predictions: {num_correct}/{num_total}\n",
    "    â””â”€â”€ Error Rate: {(1-overall_accuracy)*100:.2f}%\n",
    "\n",
    "ğŸ“ˆ Per-Class Performance:\"\"\")\n",
    "\n",
    "for i, class_name in enumerate(Config.CLASSES):\n",
    "    precision = report_dict[class_name.capitalize()]['precision']\n",
    "    recall = report_dict[class_name.capitalize()]['recall']\n",
    "    f1 = report_dict[class_name.capitalize()]['f1-score']\n",
    "    support = report_dict[class_name.capitalize()]['support']\n",
    "    print(f\"    {class_name.capitalize():12} | P: {precision:.3f} | R: {recall:.3f} | F1: {f1:.3f} | N: {int(support)}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ“‰ AUC-ROC Scores:\"\"\")\n",
    "for i, class_name in enumerate(Config.CLASSES):\n",
    "    print(f\"    {class_name.capitalize():12} | AUC: {roc_auc_scores[i]:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "ğŸ’¾ Saved Evaluation Files:\n",
    "    â”œâ”€â”€ confusion_matrix.png\n",
    "    â”œâ”€â”€ confusion_matrix_normalized.png\n",
    "    â”œâ”€â”€ classification_report.csv\n",
    "    â”œâ”€â”€ per_class_metrics.png\n",
    "    â”œâ”€â”€ roc_curves.png\n",
    "    â”œâ”€â”€ pr_curves.png\n",
    "    â””â”€â”€ confidence_analysis.png\n",
    "\n",
    "âœ… Model evaluation complete!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6eb11f",
   "metadata": {},
   "source": [
    "### Step 6 Complete! âœ…\n",
    "\n",
    "We have comprehensively evaluated our trained model:\n",
    "\n",
    "**Evaluation Completed:**\n",
    "1. âœ… Test set evaluation with all metrics\n",
    "2. âœ… Confusion matrix (raw counts and normalized)\n",
    "3. âœ… Classification report (precision, recall, F1-score)\n",
    "4. âœ… Per-class metrics visualization\n",
    "5. âœ… ROC curves with AUC scores\n",
    "6. âœ… Precision-Recall curves\n",
    "7. âœ… Prediction confidence analysis\n",
    "8. âœ… Sample prediction visualization (correct & incorrect)\n",
    "\n",
    "**Key Metrics Generated:**\n",
    "- Overall accuracy on test set\n",
    "- Per-class precision, recall, F1-score\n",
    "- AUC-ROC for each vehicle class\n",
    "- Average Precision (AP) scores\n",
    "- Confidence distribution analysis\n",
    "\n",
    "**Saved Visualizations:**\n",
    "- Confusion matrices\n",
    "- ROC and PR curves\n",
    "- Per-class metrics bar/radar charts\n",
    "- Confidence analysis plots\n",
    "\n",
    "**Next Step:** Step 7 - Vehicle Detection with Bounding Boxes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26559a",
   "metadata": {},
   "source": [
    "## Step 7: Vehicle Detection with Bounding Boxes\n",
    "\n",
    "Now we'll implement the complete detection pipeline that:\n",
    "1. **Detects vehicles** in images using sliding window approach\n",
    "2. **Draws bounding boxes** around detected vehicles\n",
    "3. **Classifies vehicle type** (car, truck, bus, motorcycle)\n",
    "4. **Displays confidence scores** next to each detection\n",
    "\n",
    "### Detection Approach:\n",
    "Since our CNN is trained for classification, we'll use:\n",
    "- **Sliding Window**: Scan image at multiple scales\n",
    "- **Selective Search / Region Proposals**: Find candidate regions\n",
    "- **Non-Maximum Suppression (NMS)**: Remove duplicate detections\n",
    "\n",
    "This approach creates a simple object detection pipeline from our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132528c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vehicle Detector Class\n",
    "class VehicleDetector:\n",
    "    \"\"\"\n",
    "    Vehicle detection and classification using our trained CNN.\n",
    "    Uses sliding window approach with multi-scale detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, classes, input_size=(224, 224), confidence_threshold=0.7):\n",
    "        \"\"\"\n",
    "        Initialize the detector.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained Keras model for classification\n",
    "            classes: List of class names\n",
    "            input_size: Model input size (height, width)\n",
    "            confidence_threshold: Minimum confidence to consider a detection\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.classes = classes\n",
    "        self.input_size = input_size\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        \n",
    "        # Colors for each class (BGR format for OpenCV)\n",
    "        self.colors = {\n",
    "            'car': (0, 255, 0),        # Green\n",
    "            'truck': (255, 0, 0),       # Blue\n",
    "            'bus': (0, 165, 255),       # Orange\n",
    "            'motorcycle': (255, 0, 255)  # Magenta\n",
    "        }\n",
    "        \n",
    "    def preprocess_region(self, region):\n",
    "        \"\"\"Preprocess a region for model input.\"\"\"\n",
    "        # Resize to model input size\n",
    "        resized = cv2.resize(region, self.input_size)\n",
    "        # Normalize to [0, 1]\n",
    "        normalized = resized.astype(np.float32) / 255.0\n",
    "        # Add batch dimension\n",
    "        return np.expand_dims(normalized, axis=0)\n",
    "    \n",
    "    def classify_region(self, region):\n",
    "        \"\"\"\n",
    "        Classify a region and return class and confidence.\n",
    "        \"\"\"\n",
    "        preprocessed = self.preprocess_region(region)\n",
    "        predictions = self.model.predict(preprocessed, verbose=0)[0]\n",
    "        \n",
    "        class_idx = np.argmax(predictions)\n",
    "        confidence = predictions[class_idx]\n",
    "        class_name = self.classes[class_idx]\n",
    "        \n",
    "        return class_name, confidence, predictions\n",
    "    \n",
    "    def sliding_window(self, image, step_size, window_size):\n",
    "        \"\"\"\n",
    "        Generate sliding windows over an image.\n",
    "        \"\"\"\n",
    "        for y in range(0, image.shape[0] - window_size[1], step_size):\n",
    "            for x in range(0, image.shape[1] - window_size[0], step_size):\n",
    "                yield (x, y, image[y:y + window_size[1], x:x + window_size[0]])\n",
    "    \n",
    "    def pyramid(self, image, scale=1.5, min_size=(100, 100)):\n",
    "        \"\"\"\n",
    "        Generate image pyramid for multi-scale detection.\n",
    "        \"\"\"\n",
    "        yield image\n",
    "        \n",
    "        while True:\n",
    "            w = int(image.shape[1] / scale)\n",
    "            h = int(image.shape[0] / scale)\n",
    "            \n",
    "            if w < min_size[0] or h < min_size[1]:\n",
    "                break\n",
    "                \n",
    "            image = cv2.resize(image, (w, h))\n",
    "            yield image\n",
    "\n",
    "print(\"âœ… VehicleDetector class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df37b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Maximum Suppression (NMS) for removing overlapping detections\n",
    "def non_max_suppression(boxes, scores, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Apply Non-Maximum Suppression to remove overlapping bounding boxes.\n",
    "    \n",
    "    Args:\n",
    "        boxes: List of bounding boxes [(x1, y1, x2, y2), ...]\n",
    "        scores: List of confidence scores\n",
    "        threshold: IoU threshold for suppression\n",
    "    \n",
    "    Returns:\n",
    "        Indices of boxes to keep\n",
    "    \"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    \n",
    "    boxes = np.array(boxes)\n",
    "    scores = np.array(scores)\n",
    "    \n",
    "    # Get coordinates\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "    \n",
    "    # Calculate area\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    \n",
    "    # Sort by scores\n",
    "    order = scores.argsort()[::-1]\n",
    "    \n",
    "    keep = []\n",
    "    \n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(i)\n",
    "        \n",
    "        # Calculate IoU with remaining boxes\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        \n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        \n",
    "        inter = w * h\n",
    "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "        \n",
    "        # Keep boxes with IoU less than threshold\n",
    "        inds = np.where(iou <= threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    \n",
    "    return keep\n",
    "\n",
    "print(\"âœ… Non-Maximum Suppression function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5411db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Detection Function with Bounding Boxes\n",
    "def detect_vehicles_sliding_window(image, detector, window_sizes=[(128, 128), (192, 192), (256, 256)],\n",
    "                                   step_ratio=0.25, nms_threshold=0.4):\n",
    "    \"\"\"\n",
    "    Detect vehicles using sliding window at multiple scales.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (RGB)\n",
    "        detector: VehicleDetector instance\n",
    "        window_sizes: List of window sizes to use\n",
    "        step_ratio: Step size as ratio of window size\n",
    "        nms_threshold: NMS IoU threshold\n",
    "    \n",
    "    Returns:\n",
    "        List of detections [(bbox, class_name, confidence), ...]\n",
    "    \"\"\"\n",
    "    detections = []\n",
    "    \n",
    "    h, w = image.shape[:2]\n",
    "    \n",
    "    for win_w, win_h in window_sizes:\n",
    "        step_x = int(win_w * step_ratio)\n",
    "        step_y = int(win_h * step_ratio)\n",
    "        \n",
    "        for y in range(0, h - win_h, step_y):\n",
    "            for x in range(0, w - win_w, step_x):\n",
    "                # Extract window\n",
    "                window = image[y:y+win_h, x:x+win_w]\n",
    "                \n",
    "                # Classify\n",
    "                class_name, confidence, _ = detector.classify_region(window)\n",
    "                \n",
    "                # Keep if above threshold\n",
    "                if confidence >= detector.confidence_threshold:\n",
    "                    bbox = (x, y, x + win_w, y + win_h)\n",
    "                    detections.append({\n",
    "                        'bbox': bbox,\n",
    "                        'class': class_name,\n",
    "                        'confidence': confidence\n",
    "                    })\n",
    "    \n",
    "    # Apply NMS\n",
    "    if len(detections) > 0:\n",
    "        boxes = [d['bbox'] for d in detections]\n",
    "        scores = [d['confidence'] for d in detections]\n",
    "        \n",
    "        keep_indices = non_max_suppression(boxes, scores, nms_threshold)\n",
    "        detections = [detections[i] for i in keep_indices]\n",
    "    \n",
    "    return detections\n",
    "\n",
    "print(\"âœ… Sliding window detection function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Detections on Image\n",
    "def draw_detections(image, detections, show_confidence=True):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and labels on image.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (RGB)\n",
    "        detections: List of detections from detect_vehicles\n",
    "        show_confidence: Whether to show confidence percentages\n",
    "    \n",
    "    Returns:\n",
    "        Image with drawn detections\n",
    "    \"\"\"\n",
    "    # Make a copy\n",
    "    output = image.copy()\n",
    "    \n",
    "    # Convert to BGR for OpenCV if needed\n",
    "    if output.max() <= 1.0:\n",
    "        output = (output * 255).astype(np.uint8)\n",
    "    \n",
    "    # Colors for each class\n",
    "    colors = {\n",
    "        'car': (0, 255, 0),        # Green\n",
    "        'truck': (255, 128, 0),     # Orange\n",
    "        'bus': (0, 128, 255),       # Blue\n",
    "        'motorcycle': (255, 0, 255)  # Magenta\n",
    "    }\n",
    "    \n",
    "    for det in detections:\n",
    "        bbox = det['bbox']\n",
    "        class_name = det['class']\n",
    "        confidence = det['confidence']\n",
    "        \n",
    "        x1, y1, x2, y2 = [int(c) for c in bbox]\n",
    "        color = colors.get(class_name, (255, 255, 255))\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(output, (x1, y1), (x2, y2), color, 2)\n",
    "        \n",
    "        # Prepare label text\n",
    "        if show_confidence:\n",
    "            label = f\"{class_name.upper()}: {confidence*100:.1f}%\"\n",
    "        else:\n",
    "            label = class_name.upper()\n",
    "        \n",
    "        # Get text size\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.6\n",
    "        thickness = 2\n",
    "        (text_w, text_h), baseline = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "        \n",
    "        # Draw label background\n",
    "        cv2.rectangle(output, (x1, y1 - text_h - 10), (x1 + text_w + 5, y1), color, -1)\n",
    "        \n",
    "        # Draw label text\n",
    "        cv2.putText(output, label, (x1 + 2, y1 - 5), font, font_scale, (255, 255, 255), thickness)\n",
    "    \n",
    "    return output\n",
    "\n",
    "print(\"âœ… Draw detections function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31511df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Detection for Pre-cropped Images (Classification + Box)\n",
    "def detect_and_classify_vehicle(image, model, classes, draw_box=True):\n",
    "    \"\"\"\n",
    "    Classify a vehicle image and draw bounding box with label.\n",
    "    \n",
    "    This is for images that already contain a single vehicle.\n",
    "    Draws a box around the entire image with classification result.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (RGB, normalized or uint8)\n",
    "        model: Trained classification model\n",
    "        classes: List of class names\n",
    "        draw_box: Whether to draw the bounding box\n",
    "    \n",
    "    Returns:\n",
    "        output_image: Image with detection drawn\n",
    "        class_name: Predicted class\n",
    "        confidence: Prediction confidence\n",
    "    \"\"\"\n",
    "    # Ensure image is in correct format\n",
    "    if image.max() <= 1.0:\n",
    "        img_display = (image * 255).astype(np.uint8)\n",
    "        img_input = image\n",
    "    else:\n",
    "        img_display = image.astype(np.uint8)\n",
    "        img_input = image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Resize for model if needed\n",
    "    h, w = img_input.shape[:2]\n",
    "    img_resized = cv2.resize(img_input, (Config.IMG_WIDTH, Config.IMG_HEIGHT))\n",
    "    \n",
    "    # Predict\n",
    "    img_batch = np.expand_dims(img_resized, axis=0)\n",
    "    predictions = model.predict(img_batch, verbose=0)[0]\n",
    "    \n",
    "    class_idx = np.argmax(predictions)\n",
    "    confidence = predictions[class_idx]\n",
    "    class_name = classes[class_idx]\n",
    "    \n",
    "    # Draw on image\n",
    "    output = img_display.copy()\n",
    "    \n",
    "    if draw_box:\n",
    "        # Colors\n",
    "        colors = {\n",
    "            'car': (0, 255, 0),\n",
    "            'truck': (255, 128, 0),\n",
    "            'bus': (0, 128, 255),\n",
    "            'motorcycle': (255, 0, 255)\n",
    "        }\n",
    "        color = colors.get(class_name, (0, 255, 0))\n",
    "        \n",
    "        # Draw box around entire image with padding\n",
    "        padding = 5\n",
    "        cv2.rectangle(output, (padding, padding), (w-padding, h-padding), color, 3)\n",
    "        \n",
    "        # Label\n",
    "        label = f\"{class_name.upper()}: {confidence*100:.1f}%\"\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.7\n",
    "        thickness = 2\n",
    "        (text_w, text_h), _ = cv2.getTextSize(label, font, font_scale, thickness)\n",
    "        \n",
    "        # Draw label background\n",
    "        cv2.rectangle(output, (padding, padding), (padding + text_w + 10, padding + text_h + 15), color, -1)\n",
    "        \n",
    "        # Draw label text\n",
    "        cv2.putText(output, label, (padding + 5, padding + text_h + 5), font, font_scale, (255, 255, 255), thickness)\n",
    "    \n",
    "    return output, class_name, confidence\n",
    "\n",
    "print(\"âœ… Single vehicle detection function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f9cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Detection Pipeline Function\n",
    "def run_full_detection_pipeline(image_path, model, detector=None, window_sizes=None, stride=32):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Load image, detect vehicles, draw boxes, show results.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to input image\n",
    "        model: Trained model\n",
    "        detector: Optional VehicleDetector instance\n",
    "        window_sizes: List of window sizes for sliding window\n",
    "        stride: Stride for sliding window\n",
    "    \n",
    "    Returns:\n",
    "        result: Detection result dictionary\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        print(f\"âŒ Could not load image: {image_path}\")\n",
    "        return None\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    if window_sizes is None:\n",
    "        window_sizes = [(128, 128), (192, 192), (256, 256)]\n",
    "    \n",
    "    # Create detector if not provided\n",
    "    if detector is None:\n",
    "        detector = VehicleDetector(model, Config.CLASSES, img_size=(Config.IMG_HEIGHT, Config.IMG_WIDTH))\n",
    "    \n",
    "    print(f\"ğŸ“· Processing image: {image_path}\")\n",
    "    print(f\"   Image size: {img_rgb.shape[1]}x{img_rgb.shape[0]}\")\n",
    "    \n",
    "    # Detect vehicles\n",
    "    detections = detect_vehicles_sliding_window(\n",
    "        img_rgb, model, Config.CLASSES,\n",
    "        window_sizes=window_sizes,\n",
    "        stride=stride,\n",
    "        confidence_threshold=0.7,\n",
    "        nms_threshold=0.3\n",
    "    )\n",
    "    \n",
    "    print(f\"   Found {len(detections)} vehicle(s)\")\n",
    "    \n",
    "    # Draw detections\n",
    "    output_image = draw_detections(img_rgb, detections)\n",
    "    \n",
    "    # Create result dictionary\n",
    "    result = {\n",
    "        'image_path': image_path,\n",
    "        'original_image': img_rgb,\n",
    "        'output_image': output_image,\n",
    "        'detections': detections,\n",
    "        'num_detections': len(detections)\n",
    "    }\n",
    "    \n",
    "    # Print detection details\n",
    "    for i, det in enumerate(detections):\n",
    "        print(f\"   Detection {i+1}: {det['class_name']} ({det['confidence']*100:.1f}%)\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"âœ… Full detection pipeline function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713f6c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Detection on Validation/Test Images\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“¦ TESTING VEHICLE DETECTION ON SAMPLE IMAGES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get some test images\n",
    "if 'val_generator' in dir() and val_generator is not None:\n",
    "    # Use validation data\n",
    "    test_images, test_labels = next(iter(val_generator))\n",
    "    print(f\"âœ… Loaded {len(test_images)} images from validation set\")\n",
    "elif 'X_val' in dir() and X_val is not None:\n",
    "    test_images = X_val[:8]\n",
    "    test_labels = y_val[:8] if 'y_val' in dir() else None\n",
    "    print(f\"âœ… Loaded {len(test_images)} images from validation array\")\n",
    "else:\n",
    "    # Generate synthetic test images\n",
    "    print(\"âš ï¸ No validation data found. Generating synthetic test images...\")\n",
    "    test_images = np.random.rand(8, Config.IMG_HEIGHT, Config.IMG_WIDTH, 3).astype(np.float32)\n",
    "    test_labels = None\n",
    "\n",
    "# Test single image classification with bounding box\n",
    "print(\"\\nğŸ” Testing single image classification with bounding boxes...\")\n",
    "\n",
    "# Ensure model is available\n",
    "if 'model' not in dir() or model is None:\n",
    "    print(\"âš ï¸ Model not trained yet. Building fresh model for demonstration...\")\n",
    "    model = build_vehicle_cnn(input_shape=(Config.IMG_HEIGHT, Config.IMG_WIDTH, 3), \n",
    "                              num_classes=len(Config.CLASSES))\n",
    "\n",
    "# Create detector instance\n",
    "detector = VehicleDetector(model, Config.CLASSES, \n",
    "                          img_size=(Config.IMG_HEIGHT, Config.IMG_WIDTH),\n",
    "                          confidence_threshold=0.5)\n",
    "\n",
    "print(\"âœ… Detector initialized!\")\n",
    "print(f\"   Classes: {Config.CLASSES}\")\n",
    "print(f\"   Input size: {Config.IMG_HEIGHT}x{Config.IMG_WIDTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7524cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Detection Results on Sample Images\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "print(\"ğŸ¨ Visualizing detection results on sample images...\\n\")\n",
    "\n",
    "for idx, ax in enumerate(axes):\n",
    "    if idx < len(test_images):\n",
    "        img = test_images[idx]\n",
    "        \n",
    "        # Apply detection\n",
    "        output, class_name, confidence = detect_and_classify_vehicle(\n",
    "            img, model, Config.CLASSES, draw_box=True\n",
    "        )\n",
    "        \n",
    "        # Get ground truth if available\n",
    "        if test_labels is not None:\n",
    "            if len(test_labels.shape) > 1:  # One-hot encoded\n",
    "                true_class = Config.CLASSES[np.argmax(test_labels[idx])]\n",
    "            else:\n",
    "                true_class = Config.CLASSES[int(test_labels[idx])]\n",
    "            title = f\"Pred: {class_name} ({confidence*100:.1f}%)\\nTrue: {true_class}\"\n",
    "        else:\n",
    "            title = f\"Pred: {class_name} ({confidence*100:.1f}%)\"\n",
    "        \n",
    "        ax.imshow(output)\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"Vehicle Detection Results - Classification with Bounding Boxes\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Config.OUTPUT_DIR, 'detection_results_grid.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Detection visualization saved to 'outputs/detection_results_grid.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f74537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection Statistics and Summary\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š DETECTION STATISTICS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze detection results\n",
    "detection_results = []\n",
    "class_counts = {cls: 0 for cls in Config.CLASSES}\n",
    "confidence_by_class = {cls: [] for cls in Config.CLASSES}\n",
    "\n",
    "for idx in range(len(test_images)):\n",
    "    img = test_images[idx]\n",
    "    _, class_name, confidence = detect_and_classify_vehicle(\n",
    "        img, model, Config.CLASSES, draw_box=False\n",
    "    )\n",
    "    \n",
    "    detection_results.append({\n",
    "        'image_idx': idx,\n",
    "        'predicted_class': class_name,\n",
    "        'confidence': confidence\n",
    "    })\n",
    "    \n",
    "    class_counts[class_name] += 1\n",
    "    confidence_by_class[class_name].append(confidence)\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nğŸ“ˆ Detection Distribution:\")\n",
    "for cls in Config.CLASSES:\n",
    "    count = class_counts[cls]\n",
    "    avg_conf = np.mean(confidence_by_class[cls]) if confidence_by_class[cls] else 0\n",
    "    print(f\"   {cls.upper():12s}: {count:3d} detections, Avg Confidence: {avg_conf*100:.1f}%\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Overall Statistics:\")\n",
    "all_confidences = [r['confidence'] for r in detection_results]\n",
    "print(f\"   Total Detections: {len(detection_results)}\")\n",
    "print(f\"   Average Confidence: {np.mean(all_confidences)*100:.1f}%\")\n",
    "print(f\"   Min Confidence: {np.min(all_confidences)*100:.1f}%\")\n",
    "print(f\"   Max Confidence: {np.max(all_confidences)*100:.1f}%\")\n",
    "\n",
    "# Create pie chart of class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Class distribution pie chart\n",
    "colors = ['#2ecc71', '#e67e22', '#3498db', '#9b59b6']\n",
    "non_zero_classes = [(cls, count) for cls, count in class_counts.items() if count > 0]\n",
    "if non_zero_classes:\n",
    "    labels, sizes = zip(*non_zero_classes)\n",
    "    axes[0].pie(sizes, labels=[l.upper() for l in labels], autopct='%1.1f%%', \n",
    "                colors=colors[:len(labels)], startangle=90)\n",
    "    axes[0].set_title('Detection Class Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Confidence distribution histogram\n",
    "axes[1].hist(all_confidences, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(np.mean(all_confidences), color='red', linestyle='--', label=f'Mean: {np.mean(all_confidences)*100:.1f}%')\n",
    "axes[1].set_xlabel('Confidence Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Confidence Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Config.OUTPUT_DIR, 'detection_statistics.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Statistics visualization saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e110e",
   "metadata": {},
   "source": [
    "## âœ… Step 7 Complete: Vehicle Detection with Bounding Boxes\n",
    "\n",
    "### What We Implemented:\n",
    "\n",
    "1. **VehicleDetector Class**: Complete detection pipeline encapsulated in a reusable class\n",
    "   - Preprocessing and inference\n",
    "   - Multi-scale sliding window detection\n",
    "   - Non-Maximum Suppression (NMS)\n",
    "\n",
    "2. **Detection Functions**:\n",
    "   - `detect_vehicles_sliding_window()`: Multi-scale sliding window for detecting vehicles in larger images\n",
    "   - `non_max_suppression()`: Remove overlapping detections\n",
    "   - `draw_detections()`: Draw bounding boxes with class labels and confidence scores\n",
    "   - `detect_and_classify_vehicle()`: Simple classification for pre-cropped vehicle images\n",
    "\n",
    "3. **Visualization**:\n",
    "   - Bounding boxes color-coded by vehicle type\n",
    "   - Class label and confidence percentage displayed\n",
    "   - Detection statistics and distribution analysis\n",
    "\n",
    "### Detection Approach:\n",
    "Since our CNN is trained for **classification** (identifying vehicle types), we use a **sliding window approach** for detection:\n",
    "- Scan the image with windows of multiple sizes\n",
    "- Classify each window region\n",
    "- Apply Non-Maximum Suppression to remove duplicates\n",
    "- Draw final bounding boxes with labels\n",
    "\n",
    "### Color Coding:\n",
    "- ğŸŸ¢ **Car**: Green\n",
    "- ğŸŸ  **Truck**: Orange  \n",
    "- ğŸ”µ **Bus**: Blue\n",
    "- ğŸŸ£ **Motorcycle**: Purple\n",
    "\n",
    "---\n",
    "**Next Step**: Step 8 - Complete Inference Pipeline and Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43730405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸš— Step 8: Complete Inference Pipeline and Demo\n",
    "\n",
    "In this final step, we will:\n",
    "1. Create a complete, production-ready inference pipeline\n",
    "2. Build functions for processing single images and batches\n",
    "3. Implement video frame processing (optional)\n",
    "4. Create an interactive demo\n",
    "5. Export the final model\n",
    "6. Summarize the complete project\n",
    "\n",
    "This step brings everything together into a usable vehicle detection system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051bcd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Vehicle Detection Inference Class\n",
    "class VehicleDetectionSystem:\n",
    "    \"\"\"\n",
    "    Complete Vehicle Detection System - Production Ready\n",
    "    \n",
    "    This class encapsulates the entire detection pipeline for easy deployment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path=None, model=None, classes=None):\n",
    "        \"\"\"\n",
    "        Initialize the detection system.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to saved model (.h5 or SavedModel format)\n",
    "            model: Pre-loaded Keras model (alternative to model_path)\n",
    "            classes: List of class names\n",
    "        \"\"\"\n",
    "        self.classes = classes or Config.CLASSES\n",
    "        self.img_size = (Config.IMG_HEIGHT, Config.IMG_WIDTH)\n",
    "        \n",
    "        # Load model\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            print(\"âœ… Model loaded from memory\")\n",
    "        elif model_path is not None:\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "            print(f\"âœ… Model loaded from: {model_path}\")\n",
    "        else:\n",
    "            print(\"âš ï¸ No model provided. Building new model...\")\n",
    "            self.model = build_vehicle_cnn(\n",
    "                input_shape=(self.img_size[0], self.img_size[1], 3),\n",
    "                num_classes=len(self.classes)\n",
    "            )\n",
    "        \n",
    "        # Color map for visualization\n",
    "        self.colors = {\n",
    "            'car': (0, 255, 0),        # Green\n",
    "            'truck': (255, 128, 0),    # Orange\n",
    "            'bus': (0, 128, 255),      # Blue\n",
    "            'motorcycle': (255, 0, 255) # Purple\n",
    "        }\n",
    "        \n",
    "        print(f\"ğŸ“Š Classes: {self.classes}\")\n",
    "        print(f\"ğŸ–¼ï¸ Input size: {self.img_size}\")\n",
    "    \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"Preprocess image for model input.\"\"\"\n",
    "        if isinstance(image, str):\n",
    "            # Load from path\n",
    "            img = cv2.imread(image)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Could not load image: {image}\")\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            img = image.copy()\n",
    "        \n",
    "        # Ensure correct format\n",
    "        if img.max() > 1.0:\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Resize\n",
    "        img_resized = cv2.resize(img, self.img_size)\n",
    "        \n",
    "        return img_resized\n",
    "    \n",
    "    def classify(self, image):\n",
    "        \"\"\"\n",
    "        Classify a single vehicle image.\n",
    "        \n",
    "        Returns:\n",
    "            dict: {class_name, confidence, all_probabilities}\n",
    "        \"\"\"\n",
    "        img = self.preprocess_image(image)\n",
    "        img_batch = np.expand_dims(img, axis=0)\n",
    "        \n",
    "        predictions = self.model.predict(img_batch, verbose=0)[0]\n",
    "        class_idx = np.argmax(predictions)\n",
    "        \n",
    "        return {\n",
    "            'class_name': self.classes[class_idx],\n",
    "            'confidence': float(predictions[class_idx]),\n",
    "            'all_probabilities': {cls: float(prob) for cls, prob in zip(self.classes, predictions)}\n",
    "        }\n",
    "    \n",
    "    def detect(self, image, window_sizes=None, stride=32, \n",
    "               confidence_threshold=0.7, nms_threshold=0.3):\n",
    "        \"\"\"\n",
    "        Detect and classify vehicles in an image.\n",
    "        \n",
    "        Args:\n",
    "            image: Input image (path or array)\n",
    "            window_sizes: List of (width, height) tuples for sliding window\n",
    "            stride: Sliding window stride\n",
    "            confidence_threshold: Minimum confidence for detection\n",
    "            nms_threshold: IoU threshold for NMS\n",
    "        \n",
    "        Returns:\n",
    "            list: List of detection dictionaries\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            img = cv2.imread(image)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            img = image.copy()\n",
    "            if img.max() <= 1.0:\n",
    "                img = (img * 255).astype(np.uint8)\n",
    "        \n",
    "        if window_sizes is None:\n",
    "            window_sizes = [(128, 128), (192, 192), (256, 256)]\n",
    "        \n",
    "        detections = detect_vehicles_sliding_window(\n",
    "            img, self.model, self.classes,\n",
    "            window_sizes=window_sizes,\n",
    "            stride=stride,\n",
    "            confidence_threshold=confidence_threshold,\n",
    "            nms_threshold=nms_threshold\n",
    "        )\n",
    "        \n",
    "        return detections\n",
    "    \n",
    "    def detect_and_visualize(self, image, save_path=None, **detect_kwargs):\n",
    "        \"\"\"\n",
    "        Detect vehicles and return visualized image.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (output_image, detections)\n",
    "        \"\"\"\n",
    "        if isinstance(image, str):\n",
    "            img = cv2.imread(image)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            img = image.copy()\n",
    "            if img.max() <= 1.0:\n",
    "                img = (img * 255).astype(np.uint8)\n",
    "        \n",
    "        # Detect\n",
    "        detections = self.detect(img, **detect_kwargs)\n",
    "        \n",
    "        # Visualize\n",
    "        output = draw_detections(img, detections)\n",
    "        \n",
    "        # Save if path provided\n",
    "        if save_path:\n",
    "            cv2.imwrite(save_path, cv2.cvtColor(output, cv2.COLOR_RGB2BGR))\n",
    "            print(f\"ğŸ’¾ Saved result to: {save_path}\")\n",
    "        \n",
    "        return output, detections\n",
    "    \n",
    "    def process_batch(self, images, return_visualizations=False):\n",
    "        \"\"\"\n",
    "        Process a batch of images.\n",
    "        \n",
    "        Args:\n",
    "            images: List of images (paths or arrays)\n",
    "            return_visualizations: Whether to return visualized images\n",
    "        \n",
    "        Returns:\n",
    "            list: List of result dictionaries\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, img in enumerate(images):\n",
    "            try:\n",
    "                if return_visualizations:\n",
    "                    output, detections = self.detect_and_visualize(img)\n",
    "                    results.append({\n",
    "                        'index': i,\n",
    "                        'detections': detections,\n",
    "                        'visualization': output,\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "                else:\n",
    "                    detections = self.detect(img)\n",
    "                    results.append({\n",
    "                        'index': i,\n",
    "                        'detections': detections,\n",
    "                        'status': 'success'\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'index': i,\n",
    "                    'error': str(e),\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"âœ… VehicleDetectionSystem class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7992efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Processing Functions (Optional)\n",
    "def process_video_frame(frame, detection_system, **kwargs):\n",
    "    \"\"\"\n",
    "    Process a single video frame.\n",
    "    \n",
    "    Args:\n",
    "        frame: Video frame (BGR format from OpenCV)\n",
    "        detection_system: VehicleDetectionSystem instance\n",
    "        **kwargs: Additional arguments for detection\n",
    "    \n",
    "    Returns:\n",
    "        processed_frame: Frame with detections drawn\n",
    "    \"\"\"\n",
    "    # Convert BGR to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Detect and visualize\n",
    "    output, detections = detection_system.detect_and_visualize(frame_rgb, **kwargs)\n",
    "    \n",
    "    # Convert back to BGR for video output\n",
    "    output_bgr = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return output_bgr, detections\n",
    "\n",
    "\n",
    "def process_video(input_path, output_path, detection_system, \n",
    "                  max_frames=None, display_progress=True, **kwargs):\n",
    "    \"\"\"\n",
    "    Process a video file and save with detections.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input video\n",
    "        output_path: Path to save output video\n",
    "        detection_system: VehicleDetectionSystem instance\n",
    "        max_frames: Maximum frames to process (None for all)\n",
    "        display_progress: Show progress bar\n",
    "        **kwargs: Additional arguments for detection\n",
    "    \n",
    "    Returns:\n",
    "        dict: Processing statistics\n",
    "    \"\"\"\n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {input_path}\")\n",
    "    \n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    if max_frames:\n",
    "        total_frames = min(total_frames, max_frames)\n",
    "    \n",
    "    print(f\"ğŸ“¹ Processing video: {input_path}\")\n",
    "    print(f\"   Resolution: {width}x{height}\")\n",
    "    print(f\"   FPS: {fps}\")\n",
    "    print(f\"   Frames to process: {total_frames}\")\n",
    "    \n",
    "    # Setup output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Process frames\n",
    "    frame_count = 0\n",
    "    all_detections = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while cap.isOpened() and (max_frames is None or frame_count < max_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process frame\n",
    "        processed_frame, detections = process_video_frame(frame, detection_system, **kwargs)\n",
    "        \n",
    "        # Write to output\n",
    "        out.write(processed_frame)\n",
    "        \n",
    "        all_detections.append({\n",
    "            'frame': frame_count,\n",
    "            'detections': detections\n",
    "        })\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        if display_progress and frame_count % 30 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            fps_actual = frame_count / elapsed\n",
    "            print(f\"   Processed {frame_count}/{total_frames} frames ({fps_actual:.1f} fps)\")\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    stats = {\n",
    "        'input_path': input_path,\n",
    "        'output_path': output_path,\n",
    "        'frames_processed': frame_count,\n",
    "        'elapsed_time': elapsed,\n",
    "        'avg_fps': frame_count / elapsed,\n",
    "        'total_detections': sum(len(d['detections']) for d in all_detections)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nâœ… Video processing complete!\")\n",
    "    print(f\"   Output saved to: {output_path}\")\n",
    "    print(f\"   Average FPS: {stats['avg_fps']:.2f}\")\n",
    "    print(f\"   Total detections: {stats['total_detections']}\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"âœ… Video processing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4044cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Complete Detection System\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸš— INITIALIZING VEHICLE DETECTION SYSTEM\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for trained model\n",
    "model_path = os.path.join(Config.MODEL_DIR, 'vehicle_cnn_best.h5')\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    # Load trained model\n",
    "    detection_system = VehicleDetectionSystem(model_path=model_path, classes=Config.CLASSES)\n",
    "    print(\"âœ… Loaded trained model from disk\")\n",
    "elif 'model' in dir() and model is not None:\n",
    "    # Use model from training\n",
    "    detection_system = VehicleDetectionSystem(model=model, classes=Config.CLASSES)\n",
    "    print(\"âœ… Using model from training session\")\n",
    "else:\n",
    "    # Create new model for demonstration\n",
    "    print(\"âš ï¸ No trained model found. Creating fresh model for demonstration...\")\n",
    "    detection_system = VehicleDetectionSystem(classes=Config.CLASSES)\n",
    "\n",
    "print(\"\\nğŸ“Š System Ready!\")\n",
    "print(f\"   Model parameters: {detection_system.model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc786f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Single Image Classification\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ¯ DEMO: SINGLE IMAGE CLASSIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get test images\n",
    "if 'test_images' in dir() and len(test_images) > 0:\n",
    "    demo_images = test_images[:4]\n",
    "elif 'X_val' in dir() and X_val is not None and len(X_val) > 0:\n",
    "    demo_images = X_val[:4]\n",
    "else:\n",
    "    # Generate synthetic demo images\n",
    "    print(\"âš ï¸ No test data available. Generating synthetic images...\")\n",
    "    demo_images = np.random.rand(4, Config.IMG_HEIGHT, Config.IMG_WIDTH, 3).astype(np.float32)\n",
    "\n",
    "# Classify each image\n",
    "print(\"\\nğŸ” Classification Results:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "classification_results = []\n",
    "for i, img in enumerate(demo_images):\n",
    "    result = detection_system.classify(img)\n",
    "    classification_results.append(result)\n",
    "    \n",
    "    print(f\"\\nğŸ“· Image {i+1}:\")\n",
    "    print(f\"   Predicted: {result['class_name'].upper()}\")\n",
    "    print(f\"   Confidence: {result['confidence']*100:.2f}%\")\n",
    "    print(f\"   All probabilities:\")\n",
    "    for cls, prob in result['all_probabilities'].items():\n",
    "        bar = \"â–ˆ\" * int(prob * 20)\n",
    "        print(f\"      {cls:12s}: {prob*100:5.1f}% {bar}\")\n",
    "\n",
    "print(\"\\nâœ… Classification demo complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7813e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Visualize Classification Results with Bounding Boxes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (ax, img, result) in enumerate(zip(axes, demo_images, classification_results)):\n",
    "    # Draw detection on image\n",
    "    output, class_name, confidence = detect_and_classify_vehicle(\n",
    "        img, detection_system.model, Config.CLASSES, draw_box=True\n",
    "    )\n",
    "    \n",
    "    ax.imshow(output)\n",
    "    ax.set_title(f\"Predicted: {class_name.upper()}\\nConfidence: {confidence*100:.1f}%\", \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"ğŸš— Vehicle Classification Demo with Bounding Boxes\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(Config.OUTPUT_DIR, 'demo_classification.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ’¾ Demo visualization saved to 'outputs/demo_classification.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae8253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Batch Processing\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“¦ DEMO: BATCH PROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Process batch of images\n",
    "batch_results = detection_system.process_batch(demo_images, return_visualizations=True)\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in batch_results if r['status'] == 'success')\n",
    "failed = sum(1 for r in batch_results if r['status'] == 'failed')\n",
    "\n",
    "print(f\"\\nğŸ“Š Batch Processing Results:\")\n",
    "print(f\"   Total images: {len(batch_results)}\")\n",
    "print(f\"   Successful: {successful}\")\n",
    "print(f\"   Failed: {failed}\")\n",
    "\n",
    "# Display batch results\n",
    "if successful > 0:\n",
    "    fig, axes = plt.subplots(1, min(4, successful), figsize=(5*min(4, successful), 5))\n",
    "    if successful == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    vis_count = 0\n",
    "    for result in batch_results:\n",
    "        if result['status'] == 'success' and vis_count < 4:\n",
    "            axes[vis_count].imshow(result['visualization'])\n",
    "            n_det = len(result['detections'])\n",
    "            axes[vis_count].set_title(f\"Image {result['index']+1}\\n{n_det} detection(s)\")\n",
    "            axes[vis_count].axis('off')\n",
    "            vis_count += 1\n",
    "    \n",
    "    plt.suptitle(\"Batch Processing Results\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nâœ… Batch processing demo complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48911d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Prediction Function\n",
    "def predict_vehicle(image_source, show_all_probs=True):\n",
    "    \"\"\"\n",
    "    Interactive function to predict vehicle type from image.\n",
    "    \n",
    "    Args:\n",
    "        image_source: Path to image or numpy array\n",
    "        show_all_probs: Whether to show all class probabilities\n",
    "    \n",
    "    Returns:\n",
    "        None (displays results)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"ğŸš— VEHICLE DETECTION PREDICTION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess\n",
    "        if isinstance(image_source, str):\n",
    "            print(f\"ğŸ“· Loading image: {image_source}\")\n",
    "            img = cv2.imread(image_source)\n",
    "            if img is None:\n",
    "                print(f\"âŒ Error: Could not load image from {image_source}\")\n",
    "                return\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            img_rgb = image_source.copy()\n",
    "            if img_rgb.max() <= 1.0:\n",
    "                img_rgb = (img_rgb * 255).astype(np.uint8)\n",
    "            print(\"ğŸ“· Processing provided image array\")\n",
    "        \n",
    "        # Classify\n",
    "        result = detection_system.classify(img_rgb)\n",
    "        \n",
    "        # Draw detection\n",
    "        output, class_name, confidence = detect_and_classify_vehicle(\n",
    "            img_rgb, detection_system.model, Config.CLASSES, draw_box=True\n",
    "        )\n",
    "        \n",
    "        # Display\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Original image\n",
    "        axes[0].imshow(img_rgb)\n",
    "        axes[0].set_title(\"Original Image\", fontsize=12)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Detection result\n",
    "        axes[1].imshow(output)\n",
    "        axes[1].set_title(f\"Detection: {class_name.upper()} ({confidence*100:.1f}%)\", \n",
    "                         fontsize=12, fontweight='bold')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"\\nğŸ¯ PREDICTION RESULT:\")\n",
    "        print(f\"   Vehicle Type: {result['class_name'].upper()}\")\n",
    "        print(f\"   Confidence: {result['confidence']*100:.2f}%\")\n",
    "        \n",
    "        if show_all_probs:\n",
    "            print(f\"\\nğŸ“Š All Probabilities:\")\n",
    "            sorted_probs = sorted(result['all_probabilities'].items(), \n",
    "                                 key=lambda x: x[1], reverse=True)\n",
    "            for cls, prob in sorted_probs:\n",
    "                bar = \"â–ˆ\" * int(prob * 30)\n",
    "                marker = \" â† PREDICTED\" if cls == result['class_name'] else \"\"\n",
    "                print(f\"   {cls:12s}: {prob*100:6.2f}% {bar}{marker}\")\n",
    "        \n",
    "        print(\"\\nâœ… Prediction complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during prediction: {str(e)}\")\n",
    "\n",
    "print(\"âœ… Interactive prediction function defined!\")\n",
    "print(\"\\nğŸ“– Usage: predict_vehicle(image_path_or_array)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Interactive Prediction\n",
    "print(\"ğŸ” Testing interactive prediction with sample image...\\n\")\n",
    "\n",
    "# Use a sample image\n",
    "if len(demo_images) > 0:\n",
    "    sample_image = demo_images[0]\n",
    "    predict_vehicle(sample_image, show_all_probs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Model for Deployment\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ’¾ EXPORTING MODEL FOR DEPLOYMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create exports directory\n",
    "export_dir = os.path.join(Config.OUTPUT_DIR, 'exported_model')\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# 1. Save as Keras H5 format\n",
    "h5_path = os.path.join(export_dir, 'vehicle_detection_model.h5')\n",
    "detection_system.model.save(h5_path)\n",
    "print(f\"âœ… Saved Keras H5 model: {h5_path}\")\n",
    "\n",
    "# 2. Save as TensorFlow SavedModel format\n",
    "savedmodel_path = os.path.join(export_dir, 'vehicle_detection_savedmodel')\n",
    "detection_system.model.save(savedmodel_path, save_format='tf')\n",
    "print(f\"âœ… Saved TensorFlow SavedModel: {savedmodel_path}\")\n",
    "\n",
    "# 3. Save model architecture as JSON\n",
    "model_json = detection_system.model.to_json()\n",
    "json_path = os.path.join(export_dir, 'model_architecture.json')\n",
    "with open(json_path, 'w') as f:\n",
    "    f.write(model_json)\n",
    "print(f\"âœ… Saved model architecture: {json_path}\")\n",
    "\n",
    "# 4. Save class labels\n",
    "labels_path = os.path.join(export_dir, 'class_labels.txt')\n",
    "with open(labels_path, 'w') as f:\n",
    "    for i, cls in enumerate(Config.CLASSES):\n",
    "        f.write(f\"{i},{cls}\\n\")\n",
    "print(f\"âœ… Saved class labels: {labels_path}\")\n",
    "\n",
    "# 5. Save model summary\n",
    "summary_path = os.path.join(export_dir, 'model_summary.txt')\n",
    "with open(summary_path, 'w') as f:\n",
    "    detection_system.model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "print(f\"âœ… Saved model summary: {summary_path}\")\n",
    "\n",
    "# Print export summary\n",
    "print(f\"\\nğŸ“ All exports saved to: {export_dir}\")\n",
    "print(f\"\\nğŸ“Š Exported Files:\")\n",
    "for file in os.listdir(export_dir):\n",
    "    file_path = os.path.join(export_dir, file)\n",
    "    if os.path.isfile(file_path):\n",
    "        size = os.path.getsize(file_path) / 1024  # KB\n",
    "        print(f\"   - {file}: {size:.1f} KB\")\n",
    "    else:\n",
    "        print(f\"   - {file}/ (directory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Inference Script for Standalone Use\n",
    "inference_script = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Vehicle Detection Inference Script\n",
    "===================================\n",
    "Standalone script for vehicle detection using the trained CNN model.\n",
    "\n",
    "Usage:\n",
    "    python vehicle_inference.py --image path/to/image.jpg\n",
    "    python vehicle_inference.py --image path/to/image.jpg --output result.jpg\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configuration\n",
    "CLASSES = ['car', 'truck', 'bus', 'motorcycle']\n",
    "IMG_SIZE = (224, 224)\n",
    "COLORS = {\n",
    "    'car': (0, 255, 0),\n",
    "    'truck': (255, 128, 0),\n",
    "    'bus': (0, 128, 255),\n",
    "    'motorcycle': (255, 0, 255)\n",
    "}\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained model.\"\"\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"Model loaded from: {model_path}\")\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess image.\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not load image: {image_path}\")\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_resized = cv2.resize(img_rgb, IMG_SIZE)\n",
    "    img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "    \n",
    "    return img_rgb, img_normalized\n",
    "\n",
    "def predict(model, image):\n",
    "    \"\"\"Make prediction on preprocessed image.\"\"\"\n",
    "    img_batch = np.expand_dims(image, axis=0)\n",
    "    predictions = model.predict(img_batch, verbose=0)[0]\n",
    "    \n",
    "    class_idx = np.argmax(predictions)\n",
    "    confidence = predictions[class_idx]\n",
    "    class_name = CLASSES[class_idx]\n",
    "    \n",
    "    return class_name, confidence, predictions\n",
    "\n",
    "def draw_result(image, class_name, confidence):\n",
    "    \"\"\"Draw bounding box and label on image.\"\"\"\n",
    "    output = image.copy()\n",
    "    h, w = output.shape[:2]\n",
    "    color = COLORS.get(class_name, (0, 255, 0))\n",
    "    \n",
    "    # Draw box\n",
    "    padding = 5\n",
    "    cv2.rectangle(output, (padding, padding), (w-padding, h-padding), color, 3)\n",
    "    \n",
    "    # Draw label\n",
    "    label = f\"{class_name.upper()}: {confidence*100:.1f}%\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    (text_w, text_h), _ = cv2.getTextSize(label, font, 0.7, 2)\n",
    "    cv2.rectangle(output, (padding, padding), (padding + text_w + 10, padding + text_h + 15), color, -1)\n",
    "    cv2.putText(output, label, (padding + 5, padding + text_h + 5), font, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Vehicle Detection Inference')\n",
    "    parser.add_argument('--image', required=True, help='Path to input image')\n",
    "    parser.add_argument('--model', default='exported_model/vehicle_detection_model.h5', help='Path to model')\n",
    "    parser.add_argument('--output', help='Path to save output image')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(args.model)\n",
    "    \n",
    "    # Process image\n",
    "    img_original, img_processed = preprocess_image(args.image)\n",
    "    \n",
    "    # Predict\n",
    "    class_name, confidence, all_probs = predict(model, img_processed)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\\\nPrediction: {class_name.upper()}\")\n",
    "    print(f\"Confidence: {confidence*100:.2f}%\")\n",
    "    print(\"\\\\nAll probabilities:\")\n",
    "    for cls, prob in zip(CLASSES, all_probs):\n",
    "        print(f\"  {cls}: {prob*100:.2f}%\")\n",
    "    \n",
    "    # Draw and save result\n",
    "    result = draw_result(img_original, class_name, confidence)\n",
    "    \n",
    "    if args.output:\n",
    "        cv2.imwrite(args.output, cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
    "        print(f\"\\\\nResult saved to: {args.output}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save inference script\n",
    "script_path = os.path.join(export_dir, 'vehicle_inference.py')\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(inference_script)\n",
    "\n",
    "print(f\"âœ… Saved standalone inference script: {script_path}\")\n",
    "print(\"\\nğŸ“– Usage:\")\n",
    "print(f\"   python {script_path} --image path/to/image.jpg\")\n",
    "print(f\"   python {script_path} --image path/to/image.jpg --output result.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be641c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Project Summary and Statistics\n",
    "print(\"=\" * 70)\n",
    "print(\"ğŸ† PROJECT COMPLETE: CNN-BASED VEHICLE DETECTION SYSTEM\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Project statistics\n",
    "print(\"\\nğŸ“Š PROJECT STATISTICS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Model info\n",
    "total_params = detection_system.model.count_params()\n",
    "trainable_params = sum([tf.reduce_prod(w.shape).numpy() for w in detection_system.model.trainable_weights])\n",
    "non_trainable = total_params - trainable_params\n",
    "\n",
    "print(f\"\\nğŸ§  Model Architecture:\")\n",
    "print(f\"   Total Parameters: {total_params:,}\")\n",
    "print(f\"   Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"   Non-trainable Parameters: {non_trainable:,}\")\n",
    "print(f\"   Input Shape: {Config.IMG_HEIGHT}x{Config.IMG_WIDTH}x3\")\n",
    "print(f\"   Output Classes: {len(Config.CLASSES)}\")\n",
    "\n",
    "print(f\"\\nğŸš— Vehicle Classes:\")\n",
    "for i, cls in enumerate(Config.CLASSES):\n",
    "    emoji = {'car': 'ğŸš—', 'truck': 'ğŸšš', 'bus': 'ğŸšŒ', 'motorcycle': 'ğŸï¸'}\n",
    "    print(f\"   {i+1}. {emoji.get(cls, 'ğŸš™')} {cls.upper()}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Output Files:\")\n",
    "print(f\"   Model exports: {export_dir}/\")\n",
    "print(f\"   Visualizations: {Config.OUTPUT_DIR}/\")\n",
    "\n",
    "# Check if training was done\n",
    "if 'history' in dir() and history is not None:\n",
    "    final_acc = history.history.get('val_accuracy', history.history.get('accuracy', [0]))[-1]\n",
    "    final_loss = history.history.get('val_loss', history.history.get('loss', [0]))[-1]\n",
    "    print(f\"\\nğŸ“ˆ Training Results:\")\n",
    "    print(f\"   Final Accuracy: {final_acc*100:.2f}%\")\n",
    "    print(f\"   Final Loss: {final_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"âœ… Vehicle Detection System Successfully Created!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f2a54e",
   "metadata": {},
   "source": [
    "## âœ… Step 8 Complete: Inference Pipeline and Demo\n",
    "\n",
    "### What We Implemented:\n",
    "\n",
    "1. **VehicleDetectionSystem Class**: Production-ready detection system\n",
    "   - Model loading from file or memory\n",
    "   - Single image classification\n",
    "   - Multi-scale detection with sliding window\n",
    "   - Batch processing capability\n",
    "\n",
    "2. **Video Processing** (Optional):\n",
    "   - Frame-by-frame processing\n",
    "   - Output video with detections\n",
    "   - Processing statistics\n",
    "\n",
    "3. **Interactive Demo**:\n",
    "   - `predict_vehicle()` function for easy testing\n",
    "   - Visual output with bounding boxes\n",
    "   - Probability distribution display\n",
    "\n",
    "4. **Model Export**:\n",
    "   - Keras H5 format\n",
    "   - TensorFlow SavedModel format\n",
    "   - Model architecture JSON\n",
    "   - Class labels file\n",
    "   - Standalone inference script\n",
    "\n",
    "### Usage Examples:\n",
    "\n",
    "```python\n",
    "# Initialize system\n",
    "system = VehicleDetectionSystem(model_path='path/to/model.h5')\n",
    "\n",
    "# Classify single image\n",
    "result = system.classify('car_image.jpg')\n",
    "print(f\"Vehicle: {result['class_name']}, Confidence: {result['confidence']:.2%}\")\n",
    "\n",
    "# Detect and visualize\n",
    "output, detections = system.detect_and_visualize('road_image.jpg')\n",
    "\n",
    "# Interactive prediction\n",
    "predict_vehicle('my_vehicle.jpg')\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79260f3d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ‰ PROJECT COMPLETE!\n",
    "\n",
    "## CNN-Based Vehicle Detection System\n",
    "\n",
    "### Summary of All Steps:\n",
    "\n",
    "| Step | Description | Status |\n",
    "|------|-------------|--------|\n",
    "| 1 | Project Setup and Library Imports | âœ… Complete |\n",
    "| 2 | Dataset Download and Preparation | âœ… Complete |\n",
    "| 3 | Data Preprocessing and Augmentation | âœ… Complete |\n",
    "| 4 | CNN Model Architecture | âœ… Complete |\n",
    "| 5 | Model Training | âœ… Complete |\n",
    "| 6 | Model Evaluation and Metrics | âœ… Complete |\n",
    "| 7 | Vehicle Detection with Bounding Boxes | âœ… Complete |\n",
    "| 8 | Inference Pipeline and Demo | âœ… Complete |\n",
    "\n",
    "### Key Features:\n",
    "- ğŸ§  **Custom CNN trained from scratch** (no pretrained models)\n",
    "- ğŸš— **4 Vehicle Classes**: Car, Truck, Bus, Motorcycle\n",
    "- ğŸ“¦ **Bounding Box Detection** with class labels and confidence scores\n",
    "- ğŸ¨ **Color-coded visualization** by vehicle type\n",
    "- ğŸ“Š **Comprehensive evaluation** with confusion matrix, ROC curves\n",
    "- ğŸ’¾ **Export-ready** for deployment\n",
    "\n",
    "### How to Use:\n",
    "1. **Run all cells** from Step 1 to Step 8\n",
    "2. Download and place your vehicle dataset in the `data/` folder\n",
    "3. Train the model (Step 5)\n",
    "4. Use `predict_vehicle(image)` for inference\n",
    "5. Export the model for production use\n",
    "\n",
    "### Files Generated:\n",
    "- `models/` - Trained model checkpoints\n",
    "- `outputs/` - Visualizations and results\n",
    "- `outputs/exported_model/` - Deployment-ready model files\n",
    "\n",
    "---\n",
    "**Author**: Generated with Deep Learning  \n",
    "**Framework**: TensorFlow/Keras  \n",
    "**Date**: 2024"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
